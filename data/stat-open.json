[
    {
        "id": 1,
        "title": "Analysis of Environmental Factors in Stroke Incidence and Intervention Strategies",
        "background": "Stroke (commonly known as cerebral apoplexy) is one of the serious diseases currently threatening human life. Its occurrence is a prolonged process, and once contracted, it is difficult to reverse. The induction of this disease has been proven to be closely related to environmental factors, including temperature and humidity. Analyzing the environmental factors contributing to the incidence of stroke aims to assess disease risks, enable timely interventions for high-risk populations, and help healthy or sub-healthy individuals understand their risk levels for stroke, thereby allowing for self-protection. Meanwhile, establishing a data model to understand the patterns of disease incidence has practical significance for health administrative departments and medical institutions in rationally allocating medical resources, improving diagnosis and treatment environments, and configuring beds and medical supplies.",
        "problem_requirement": "Please answer the following questions based on the provided data:  \n1. Based on the basic information of patients, provide a statistical description of the affected population.  \n2. Establish a mathematical model to study the relationship between the incidence of stroke and temperature, atmospheric pressure, and relative humidity.  \n3. Refer to and collect literature on important characteristics and key indicators of high-risk populations for stroke. Combine the conclusions from questions 1 and 2 to propose early warning and intervention suggestions for high-risk populations.",
        "dataset_path": [
            "data1.xlsx",
            "data2.xlsx",
            "data3.xlsx",
            "data4.xlsx",
            "data5.xlsx"
        ],
        "dataset_description": "The data1-data4 comes from the stroke case information of various hospitals in a certain city in China from January 2007 to December 2010, as well as the corresponding daily meteorological data for the local area during the same period (data5).",
        "variable_description": [
            {
                "Sex": "1 --- Male, 2 --- Female",
                "Occupation": "1 --- Farmer, 2 --- Worker, 3 --- Retiree, 4 --- Teacher, 5 --- Fisherman, 6 --- Medical Staff, 7 --- Clerk/Office Worker, 8 -- Retired/Laid-off Personnel, Space/Blank --- Other or Missing"
            },
            {
                "Sex": "1 --- Male, 2 --- Female",
                "Occupation": "1 --- Farmer, 2 --- Worker, 3 --- Retiree, 4 --- Teacher, 5 --- Fisherman, 6 --- Medical Staff, 7 --- Clerk/Office Worker, 8 -- Retired/Laid-off Personnel, Space/Blank --- Other or Missing"
            },
            {
                "Sex": "1 --- Male, 2 --- Female",
                "Occupation": "1 --- Farmer, 2 --- Worker, 3 --- Retiree, 4 --- Teacher, 5 --- Fisherman, 6 --- Medical Staff, 7 --- Clerk/Office Worker, 8 -- Retired/Laid-off Personnel, Space/Blank --- Other or Missing"
            },
            {
                "Sex": "1 --- Male, 2 --- Female",
                "Occupation": "1 --- Farmer, 2 --- Worker, 3 --- Retiree, 4 --- Teacher, 5 --- Fisherman, 6 --- Medical Staff, 7 --- Clerk/Office Worker, 8 -- Retired/Laid-off Personnel, Space/Blank --- Other or Missing"
            },
            {
                "Date": "Day",
                "Aver (High, Low) pres": "Average (Highest, Lowest) atmospheric pressure (hPa)",
                "Aver (High, Low) temp": "Average (Highest, Lowest) temperature (°C)",
                "​​Aver (Min) RH": "Average (Minimum) Relative Humidity (%)"
            }
        ],
        "addendum": "For data1 to data4: Note:\na. In the 'Time of incidence' and 'Report time' fields, there are different date formats as well as errors (e.g., ####or blanks).\nb. Some data entries are partially missing (e.g., #### or blanks in the 'Age' field).",
        "role": [
        {
            "name": "statistical expert",
            "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
        },
        {
            "name": "data scientist",
            "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
        },
        {
            "name": "Biostatistics & Epidemiological Modeling Expert",
            "details": "As a Biostatistics & Epidemiological Modeling Expert, you specialize in analyzing healthcare data, statistical description of populations, and building rigorous mathematical models to explore disease-environment relationships. Your evaluation will focus on: 1) The thoroughness of statistical description for patient characteristics (e.g., handling missing data in 'Age' or date fields, appropriate use of descriptive metrics like mean/median for age, frequency distributions for sex/occupation, and addressing biases from data errors). 2) The rationality of the mathematical model linking stroke incidence to meteorological factors (e.g., variable selection justification—whether temperature/pressure/humidity are appropriately operationalized, consideration of lag effects or non-linear relationships, model validation methods like cross-validation, and control for confounding variables such as seasonality or population demographics). 3) The logical coherence between statistical findings, model results, and subsequent intervention proposals, ensuring conclusions are data-driven and free from over-interpretation."
        },
        {  
            "name": "Health Data Innovation & Intervention Design Specialist",
            "details": "As a Health Data Innovation & Intervention Design Specialist, you focus on identifying novel approaches to data integration, modeling, and actionable public health strategies. Your evaluation will assess: 1) Innovation in data processing (e.g., creative solutions for resolving date format errors or missing entries in 'Time of incidence'—such as imputation techniques or temporal alignment algorithms to link patient data with meteorological records from data5). 2) Novelty in modeling methodology (e.g., beyond basic linear regression—use of machine learning models like random forests or LSTM for time-series incidence data, spatiotemporal analysis if geographic variation is considered, or integration of interaction terms between meteorological factors). 3) Creativity and practicality of early warning/intervention suggestions (e.g., personalized risk stratification combining demographic high-risk traits from Q1 with meteorological triggers from Q2, use of real-time meteorological data for dynamic warning systems, or community-specific interventions tailored to high-risk occupations identified in statistical descriptions)."
        }],
        "data_num": 5,
        "source": "cumcm2012c",
        "source_type": "CUMCM"
    },
    {
        "id": 2,
        "title": "Deformation of the Ancient Pagoda",
        "background": "Due to prolonged exposure to various forces such as its own weight, temperature changes, wind loads, and occasional impacts from earthquakes and hurricanes, ancient pagodas may undergo deformations such as tilting, bending, and twisting. To protect these historical structures, cultural heritage departments need to conduct regular observations to measure various deformation parameters and formulate necessary preservation measures. An ancient pagoda, with a history of over a thousand years, is a key cultural relic under national protection. The management authority commissioned a surveying company to conduct four observations of the pagoda in July 1986, August 1996, March 2009, and March 2011.",
        "problem_requirement": "Based on the four sets of observation data provided in data1.xlsx, please address the following questions: 1. Propose a general method for determining the center coordinates of each layer of the ancient pagoda, and provide a table listing the center coordinates of each layer for each measurement. 2. Analyze the deformation conditions of the pagoda, including tilting, bending, and twisting. 3. Analyze the deformation trend of the pagoda.",
        "dataset_path": [
            "data1.xlsx"
        ],
        "dataset_description": "This dataset contains observational data from multiple years (1986, 1996, 2009, 2011) with measurements of coordinates (x, y, z) at different layers and points. The data appears to represent spatial measurements of some physical structure (possibly a tower or building), with measurements taken at different heights (layers) and positions (points). The '塔尖' (tower tip/spire) entries suggest this may be monitoring data for a tower structure.",
        "variable_description": [
            {
                "Layer": "Layer number indicating the vertical level/height of measurement",
                "Point": "Point number indicating the measurement position at each layer",
                "x/m": "X-coordinate measurement in meters",
                "y/m": "Y-coordinate measurement in meters",
                "z/m": "Z-coordinate measurement in meters (likely elevation/height)",
                "1986 observation data": "Observation data from year 1986",
                "1996 observation data": "Observation data from year 1996",
                "2009 observation data": "Observation data from year 2009",
                "2011 observation data": "Observation data from year 2011",
                "Tower Spire​": "Tower tip/spire measurements (special marker for the top of the structure)",
                "Coordinates": "Coordinate measurements (x, y, z values)"
            }
        ],
        "addendum": "",
        "data_num": 1,
        "source": "cumcm2013c",
        "source_type": "CUMCM",
        "role": [{
            "name": "statistical expert",
            "details": "As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
        },
        {
            "name": "data scientist",
            "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
        },
        {
            "name": "Geodetic Surveying and Deformation Analysis Expert",
            "details": "As a Geodetic Surveying and Deformation Analysis Expert, you specialize in precise spatial measurement, coordinate system establishment, and quantitative assessment of structural deformation. Your expertise includes evaluating the methodological rigor of center coordinate determination (e.g., whether averaging, weighted least squares, or symmetry-based fitting is used, and if it accounts for the pagoda's cross-sectional characteristics), the rationality of deformation metrics (e.g., tilting quantified by horizontal displacement of layer centers relative to a reference base, bending via curvature of the centerline, twisting via rotational differences of measurement points around the center), and the depth of error analysis (e.g., consideration of measurement uncertainty, stability of reference benchmarks, or consistency of coordinate systems across observation years). You will assess whether the reasoning logically connects raw coordinate data to deformation conclusions (e.g., justifying why a certain layer is chosen as the reference for tilting analysis) and whether trends are supported by statistical evidence (e.g., significance tests for displacement rates between 1986–1996, 1996–2009, and 2009–2011)."
        },
        {
            "name": "Structural Health Monitoring Innovation Specialist",
            "details": "As a Structural Health Monitoring Innovation Specialist, you focus on advancing traditional deformation analysis through novel methodologies, interdisciplinary integration, or predictive modeling. Your expertise includes identifying innovative approaches beyond conventional surveying techniques, such as using 3D point cloud registration or parametric modeling (e.g., B-spline curves) for center coordinate determination instead of simple arithmetic averaging; integrating environmental covariates (e.g., temperature fluctuations, historical seismic events, or wind load data mentioned in the background) to explain deformation drivers; applying time-series forecasting (e.g., ARIMA, LSTM) to predict future deformation trends; or developing visualization tools (e.g., 4D dynamic models) to intuitively display tilting, bending, and twisting over time. You will evaluate whether the solution introduces creative frameworks (e.g., Bayesian inference for uncertainty quantification, or comparative analysis of deformation modes across historical periods to isolate critical events) that enhance understanding of the pagoda’s structural behavior beyond standard descriptive analysis."
        }]
    },
    {
        "id": 3,
        "title": "Prediction of Remaining Discharge Time for Batteries",
        "background": "Lead-acid batteries are widely used as power sources in industrial, military, and daily life applications. During the constant current discharge process of a lead-acid battery, the voltage monotonically decreases with discharge time until it reaches the rated minimum protection voltage (Um, which is 9V in this problem). The relationship between voltage and time from the start of a full discharge is called the discharge curve. Determining how long the battery can continue to supply power under the current load (i.e., the remaining discharge time to reach Um at the current discharge rate) is a critical question in practical use. After prolonged use or storage, the state of charge of a fully charged battery may decay. ",
        "problem_requirement": "Please represent each discharge curve using elementary functions based on Appendix 1 from data1.xlsx and provide the mean relative error (MRE, as defined in Appendix 1) for each curve. If a new battery is discharged at rates of 30A, 40A, 50A, 60A, and 70A, and the measured voltage is 9.8V in each case, what is the remaining discharge time according to your model? Establish a mathematical model for the discharge curve at any constant discharge current between 20A and 100A, and evaluate the model's accuracy using MRE. Present the discharge curve at a discharge rate of 55A in both tabular and graphical forms. Appendix 2 from data1.xlsx records discharge data from the same battery at different states of decay, discharged from a fully charged state at the same current intensity. Predict the remaining discharge time for the battery in decay state 3 as provided in Appendix 2.",
        "dataset_path": [
            "data1.xlsx"
        ],
        "dataset_description": "data1.xlsx provides sampled data from the complete discharge curves of batteries from the same production batch, tested at different discharge rates during factory inspections.",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 1,
        "source": "cumcm2016c",
        "source_type": "CUMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Battery Discharge Kinetics and Model Validation Expert",
                "details": "As a Battery Discharge Kinetics and Model Validation Expert, you possess deep knowledge of lead-acid battery electrochemistry, discharge curve modeling, elementary function fitting, and statistical error analysis (e.g., mean relative error, MRE). Your expertise includes evaluating the physical rationality of function selections (ensuring monotonic voltage decay behavior aligns with electrochemical principles like polarization and active material depletion), validating curve-fitting accuracy through MRE calculations, and assessing the robustness of current-dependent modeling (20A–100A range). You will analyze whether the solution justifies function choices (e.g., exponential, polynomial, or combined forms) based on discharge mechanism insights, rigorously validates model generalizability across currents, and logically extends to decay states by accounting for capacity degradation effects. Your focus is on ensuring the reasoning chain—from data fitting to remaining time prediction—is scientifically sound and statistically reliable."
            },
            {
                "name": "Innovative Battery Modeling Methodologist",
                "details": "As an Innovative Battery Modeling Methodologist, you specialize in advancing traditional discharge curve modeling through creative integration of physical insights, novel function forms, or cross-domain techniques. Your expertise includes identifying innovations such as: (1) Beyond empirical fitting—incorporating electrochemical principles (e.g., Peukert’s law for current-capacity relationships, polarization dynamics) into elementary function selection; (2) Adaptive modeling for decay states—developing decay-sensitive parameters (e.g., state-of-health indices) instead of static curve fits; (3) Multi-scale generalization—designing a unified framework for arbitrary currents (20A–100A) that outperforms simple interpolation/extrapolation. You will assess if the solution introduces original approaches (e.g., hybrid physical-data models, dynamic parameterization) to improve prediction accuracy, reduce MRE, or enhance interpretability of discharge behavior across currents and decay states."
            }]
    },
    {
        "id": 4,
        "title": "Color and Substance Concentration Identification",
        "background": "Colorimetry is a commonly used method for detecting substance concentration. It involves preparing the substance to be tested into a solution, dripping it onto the surface of a specific white test paper, and allowing it to react fully to obtain a colored test paper. This colored paper is then compared with a standard color chart to determine the concentration level of the substance. However, due to individual differences in color sensitivity and observational errors, the accuracy of this method is significantly affected. With advancements in photography technology and color resolution, there is a growing need to establish a quantitative relationship between color readings and substance concentrations. This would allow the concentration of the substance to be determined simply by inputting the color readings from a photograph.",
        "problem_requirement": "Based on the provided data regarding color readings and substance concentrations, please address the following questions: 1.  Discuss whether the relationship between color readings and substance concentrations can be determined from these five datasets, and propose criteria to evaluate the quality of these five datasets. 2. For the data in Data2.xls, establish a mathematical model between color readings and substance concentrations, and provide an error analysis of the model. Explore the impact of data volume and color dimensions on the model.",
        "dataset_path": [
            "data1.xlsx",
            "data2.xlsx"
        ],
        "dataset_description": "data1.xls provides color readings for five different substances at various concentrations. ",
        "variable_description": [
            {
                "B (Blue)": "Blue channel color value",
                "G (Green)": "Green channel color value",
                "R (Red)": "Red channel color value",
                "H (Hue)": "Hue (color tone)",
                "S (Saturation)": "Color saturation"
            },
            {
                "B (Blue)": "Blue channel color value",
                "G (Green)": "Green channel color value",
                "R (Red)": "Red channel color value",
                "H (Hue)": "Hue (color tone)",
                "S (Saturation)": "Color saturation"
            }
        ],
        "addendum": "The concentration unit 'ppm' (parts per million) in the attached tables is equivalent to milligrams per liter (mg/L), representing the mass of the target substance (in milligrams) dissolved in one liter of pure water (or solvent). It is commonly used for very low concentrations. For example, a histamine concentration of 50 ppm means there are 50 milligrams of histamine per liter of solution. The term 'Water' in the tables indicates a control condition where the concentration of the target substance is zero.",
        "data_num": 2,
        "source": "cumcm2017c",
        "source_type": "CUMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Colorimetric Data Modeling and Analysis Specialist",
                "details": "As a Colorimetric Data Modeling and Analysis Specialist, you possess expertise in color science, statistical modeling, and experimental data evaluation. Your focus is on assessing the depth of analysis and rationality of reasoning in solutions. You will evaluate whether the solution thoroughly discusses dataset quality criteria (e.g., linearity of concentration-color relationships, signal-to-noise ratio, concentration range coverage, consistency across replicates, and validity of the 'Water' control group). For model development, you will check if the reasoning justifies the selection of color variables (R, G, B, H, S) based on chemical reaction mechanisms (e.g., how specific color channels correlate with substance concentration changes) and statistical significance. You will also assess the rigor of error analysis, including the choice of metrics (e.g., RMSE, R², residual distribution) and whether it accounts for potential biases (e.g., lighting variations, sensor noise). Additionally, you will verify if the discussion on data volume impact includes rational arguments about sample size sufficiency, overfitting risks, and the need for validation, ensuring the analysis connects colorimetric principles to quantitative modeling logically."
            },
            {
                "name": "Color Informatics Innovation and Method Development Expert",
                "details": "As a Color Informatics Innovation and Method Development Expert, you specialize in integrating color science with advanced computational techniques to drive quantitative analysis innovation. Your role is to evaluate the novelty and creativity of solutions in addressing the problem. You will assess if the solution goes beyond conventional linear regression by exploring innovative modeling approaches (e.g., non-linear models like polynomial regression, random forests, or neural networks tailored to color data) or novel color feature engineering (e.g., transforming raw color channels into derived metrics such as R/G ratios, H-S plane clustering, or spectral unmixing of RGB values). You will also examine if the solution innovatively addresses color dimension impact—for example, using dimensionality reduction (e.g., PCA on R/G/B/H/S) to mitigate multicollinearity or leveraging domain knowledge to prioritize biologically/chemically relevant color metrics (e.g., saturation as a proxy for reaction completeness). Additionally, you will look for creative strategies to handle data volume limitations, such as transfer learning from similar colorimetric datasets, data augmentation (e.g., simulating lighting variations), or active learning to identify critical concentration points for additional sampling. Your evaluation focuses on whether the solution introduces new perspectives that advance quantitative colorimetry beyond traditional chart comparison methods."
            }
        ]
    },
    {
        "id": 5,
        "title": "Portrait of Department Store Members",
        "background": "In the retail industry, the value of members lies in their ability to consistently provide stable sales and profits for retail operators, while also offering data support for strategy formulation. The retail industry employs various methods to attract more people to become members and to enhance member loyalty as much as possible. The current development of e-commerce has led to a continuous loss of mall members, causing significant losses for retail operators. At this point, operators need to implement targeted marketing strategies to strengthen relationships with members. For example, merchants may launch a series of promotional activities for members to maintain their loyalty. Some argue that the cost of retaining existing members is too high, but in reality, the investment required to acquire new members far exceeds the cost of measures taken to retain current ones. Improving member profiling, enhancing the refined management of existing members, regularly pushing products and services to them, and establishing stable relationships with members are effective ways for the physical retail industry to achieve better development.",
        "problem_requirement": "Please establish mathematical models to address the following issues: (1) Analyze the consumption characteristics of the store's members, compare the differences between member and non-member groups, and explain the value that member groups bring to the store. (2) Based on members' consumption patterns, establish a mathematical model that can characterize the purchasing power of each member, enabling the identification of each member's value. (3) As an important resource in the retail industry, members have a lifecycle (the entire process from joining to exiting), and their status (e.g., active or inactive) may change. Within a specific time window, establish a mathematical model to define member lifecycles and status classifications, allowing store managers to manage members more effectively. (4) Develop a mathematical model to calculate the activation rate of inactive members during their lifecycle, i.e., the likelihood of transitioning from inactive to active status. Based on actual sales data, determine the relationship model between activation rates and the store's promotional activities. (5) Cross-selling is central to the operation of shopping centers. If merchants are planning a promotional event, how should they design the activity based on members' preferences and product cross-selling rates?",
        "dataset_path": [
            "data1.xlsx",
            "data2.xlsx",
            "data3.xlsx",
            "data4.xlsx"
        ],
        "dataset_description": "The attached data provides relevant information about members of a large department store: Attachment 1 contains member information data; Attachment 2 includes sales records from recent years; Attachment 3 details member consumption records; Attachment 4 lists product information, where generally, higher-priced products yield higher profits; Attachment 5 is a data dictionary.",
        "variable_description": [
            {
                "kh": "Membership card number, the unique identifier for a member.",
                "csny": "Member's date of birth.",
                "xb": "Gender, where 0 indicates female and 1 indicates male.",
                "djsj": "Member's registration date and time."
            },
            {
                "djh": "Receipt number (Identical receipt numbers may not originate from the same transaction; it is not a unique identifier).",
                "spbm": "Product code.",
                "sj": "Product selling price.",
                "sl": "Sales quantity.",
                "je": "Transaction amount.",
                "dtime": "Date and time when the transaction occurred.",
                "syjh": "Cash register number."
            },
            {
                "dtime": "Date and time when the member's consumption occurred.",
                "spbm": "Product code.",
                "sl": "Sales quantity.",
                "sj": "Product selling price.",
                "je": "Transaction amount.",
                "spmc": "Product name.",
                "jf": "Membership points earned from this transaction.",
                "syjh": "Cash register number.",
                "djh": "Receipt number (Identical receipt numbers may not originate from the same transaction).",
                "gzbm": "Counter group code.",
                "gzmc": "Counter group name."
            },
            {
                "spbm": "Product code.",
                "spmc": "Product name.",
                "splm": "Product category.",
                "ppbm": "Brand code.",
                "lbbm": "Category code.",
                "jxs": "Input tax.",
                "xxs": "Output tax.",
                "wsjj": "Pre-tax purchase price.",
                "hsjj": "Purchase price including tax.",
                "sj": "Selling price.",
                "jlrq": "Record date.",
                "csbm": "Manufacturer code.",
                "gzbm": "Counter group code."
            }
        ],
        "addendum": "",
        "data_num": 4,
        "source": "cumcm2018c",
        "source_type": "CUMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Retail Customer Analytics and Modeling Specialist",
                "details": "As a Retail Customer Analytics and Modeling Specialist, you possess expertise in statistical analysis, consumer behavior modeling, and data-driven decision validation. Your focus is on evaluating the depth and rationality of solutions by assessing whether the mathematical models are grounded in the provided datasets (member info, sales records, consumption data, product details), use appropriate variables (e.g., RFM indicators from purchase recency/frequency/monetary value in data2/data3, demographic features in data1), and adhere to logical consistency. For instance, when analyzing consumption characteristics (Problem 1), you verify if statistical tests (t-tests, ANOVA) are used to compare member vs. non-member groups; for purchasing power modeling (Problem 2), you check if RFM segmentation, lifetime value (LTV) formulas, or regression models appropriately integrate variables like transaction frequency (from data3's 'dtime' and 'je'), product category preferences (via data4's 'splm'), and member tenure (from data1's 'djsj'). For lifecycle/status classification (Problem 3), you evaluate if the time window definition aligns with retail industry standards (e.g., 3/6/12-month inactivity thresholds) and if survival analysis or time-series segmentation methods are logically applied to data3's consumption timestamps. You also validate whether activation rate models (Problem 4) control for confounding variables (e.g., promotion type, member demographics in data1) and if cross-selling strategies (Problem 5) rationally link product cross-selling rates (from data2/data3's co-purchase patterns) to member preferences (derived from data3's 'spbm' and data4's 'splm')."
            },
            {
                "name": "Retail Data Science Innovation and Advanced Analytics Expert",
                "details": "As a Retail Data Science Innovation and Advanced Analytics Expert, you specialize in applying cutting-edge analytical techniques to solve retail member management challenges, focusing on innovation beyond traditional methods. Your evaluation centers on whether solutions introduce novel approaches to address the problem requirements. For example, in purchasing power modeling (Problem 2), you assess if the solution goes beyond basic RFM by incorporating sequential purchase behavior (via time-series clustering on data3's 'dtime' sequences) or lifetime value (LTV) with probabilistic forecasting (e.g., Bayesian models). For member lifecycle/status classification (Problem 3), you check if dynamic status transitions are modeled using hidden Markov models (HMMs) or state-space models to capture probabilistic shifts between active/inactive states, rather than static threshold-based rules. For activation rate modeling (Problem 4), you evaluate the use of causal inference (e.g., propensity score matching) to isolate promotion effects from data2's 'dtime' and promotion records, or machine learning classifiers (e.g., gradient boosting) with feature engineering (e.g., recency-frequency interaction terms from data3). For cross-selling promotion design (Problem 5), you look for innovation like graph neural networks (GNNs) to model product association networks (from data2/data3's co-purchases) or deep learning-based preference prediction (e.g., using purchase sequence data to forecast category preferences). You also assess if the solution integrates multi-source data creatively (e.g., combining member demographics from data1 with product profitability from data4 to prioritize high-value cross-selling pairs)."
            }
        ]
    },
    {
        "id": 6,
        "title": "Credit Decision-making for Small and Medium-sized Enterprises",
        "background": "In practice, due to the relatively small scale of small and medium-sized enterprises (SMEs) and their lack of collateral assets, banks typically extend loans to enterprises with strong capabilities and stable supply-demand relationships based on credit policies, transaction invoice information, and the influence of upstream and downstream enterprises. Additionally, preferential interest rates may be offered to enterprises with high credibility and low credit risk. Banks first assess the credit risk of SMEs based on their strength and credibility, and then determine whether to grant loans, as well as the loan amount, interest rate, and term, based on factors such as credit risk. For enterprises approved for loans, a certain bank sets the loan amount in units of 10,000 yuan; the annual interest rate ranges from 4% to 15%; and the loan term is 1 year.",
        "problem_requirement": "The bank requests your team to study the credit strategy for SMEs by establishing mathematical models based on practical considerations and the data in the datas, primarily addressing the following issues: (1) Conduct a quantitative analysis of the credit risk of the 123 enterprises in data1 and propose a credit strategy for these enterprises under a fixed annual credit quota. (2) Based on the findings from Question 1, perform a quantitative analysis of the credit risk of the 302 enterprises in data2 and propose a credit strategy for these enterprises under an annual credit quota of 100 million yuan. (3) The production, operation, and economic efficiency of enterprises may be affected by unexpected factors, which often impact different industries and categories of enterprises differently. Considering the credit risks of the enterprises in data2 and the potential impact of unexpected factors (e.g., the COVID-19 pandemic) on each enterprise, propose an adjusted credit strategy for the bank under an annual credit quota of 100 million yuan. data1: Relevant data of 123 enterprises with credit records data2: Relevant data of 302 enterprises without credit records data3: 2019 statistical data on the relationship between bank loan interest rates and customer churn rates Data descriptions in the datas: (1) Input invoice: An invoice issued by the seller when an enterprise purchases products. (2) Output invoice: An invoice issued by an enterprise when selling products to a buyer. (3) Valid invoice: An invoice issued for normal transaction activities. (4) Voided invoice: An invoice canceled due to the termination of a transaction after its issuance. (5) Negative invoice: An invoice issued when a buyer returns goods and receives a refund after the original invoice has been recorded for taxation. (6) Credit rating: Manually assessed by the bank based on the actual situation of the enterprise. The bank generally does not extend loans to enterprises with a credit rating of D. (7) Customer churn rate: The rate at which the bank loses potential customers due to factors such as loan interest rates.",
        "dataset_path": [
            "data1.xlsx",
            "data2.xlsx",
            "data3.xlsx"
        ],
        "dataset_description": "data 1-3 provide relevant data for 123 enterprises with credit records, 302 enterprises without credit records, and 2019 statistical data on the relationship between loan interest rates and customer churn rates, respectively.",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 3,
        "source": "cumcm2020c",
        "source_type": "CUMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "SME Credit Risk Quantification & Strategy Validation Expert",
                "details": "As an expert in SME credit risk quantification and strategy validation, you possess deep knowledge of credit risk modeling, financial data analytics, and SME operational characteristics. Your expertise includes evaluating the rationality of risk metrics (e.g., probability of default, loss given default), the validity of using invoice data (input/output volume, valid/voided/negative invoice ratios) to assess transaction stability and repayment capacity, and the integration of manual credit ratings (e.g., excluding D-rated enterprises). You will analyze whether the depth of risk analysis covers critical factors (enterprise strength, supply-demand relationships, upstream/downstream influence) and if the reasoning logically connects risk assessment to loan decisions (amount, interest rate, term) under fixed credit quotas. Additionally, you will validate if the strategy optimizes the risk-return tradeoff (e.g., portfolio diversification, avoiding over-concentration in high-risk industries) and ensures alignment with practical banking constraints (e.g., 10,000 yuan loan units, 4%-15% interest rate range)."
            },
            {
                "name": "Financial Innovation & Adaptive Risk Strategy Expert",
                "details": "As an expert in financial innovation and adaptive risk strategy, you specialize in developing novel approaches to credit decision-making, especially for SMEs with limited credit history or under uncertainty. Your expertise includes evaluating the use of innovative methods to address challenges like data scarcity (for enterprises in data2 without credit records), modeling unexpected shocks (e.g., industry-specific impacts of events like the COVID-19 pandemic), and integrating non-traditional risk factors. You will assess if the solution introduces creative techniques such as: leveraging alternative data (e.g., invoice network analysis to capture upstream/downstream influence) for enterprises with no credit records; using scenario-based or stress-testing models to quantify unexpected factor impacts; dynamically adjusting interest rates by integrating customer churn rate data (data3) with risk levels; or applying machine learning/transfer learning to extrapolate insights from data1 to data2. Additionally, you will evaluate if the adjusted credit strategy under uncertainty demonstrates adaptive mechanisms (e.g., real-time risk monitoring, flexible quota reallocation across industries) that go beyond traditional static credit scoring frameworks."
            }
        ]
    },
    {
        "id": 7,
        "title": "Evaluation of Rainfall Forecasting Methods",
        "background": "Rainfall forecasting plays a crucial role in agricultural production and urban activities. However, providing accurate and timely rainfall predictions remains a challenging task that has garnered worldwide attention. A local meteorological observatory and research institute in China are studying 6-hour rainfall forecasting methods. Specifically, every day at 20:00, they forecast rainfall for four subsequent time intervals (21:00–03:00, 03:00–09:00, 09:00–15:00, and 15:00–21:00) at 53×47 grid points near longitude 120°E and latitude 32°N. Meanwhile, 91 observation stations are set up to measure the actual rainfall during these intervals. Due to various constraints, the distribution of these stations is uneven.",
        "problem_requirement": "(1) Please establish a mathematical model to evaluate the accuracy of the two 6-hour rainfall forecasting methods.  \n(2) The meteorological department classifies 6-hour rainfall into six levels: 0.1–2.5 mm as light rain, 2.6–6 mm as moderate rain, 6.1–12 mm as heavy rain, 12.1–25 mm as rainstorm, 25.1–60 mm as heavy rainstorm, and greater than 60.1 mm as extreme rainstorm. If forecasts are provided to the public based on this classification, how should public perception be considered in the evaluation method?",
        "dataset_path": [
            "FORECAST",
            "MEASURING"
        ],
        "dataset_description": "In the FORECAST folder, the files lon.dat and lat.dat contain the longitude and latitude of the grid points, respectively. The remaining files are named <fdatei>_dis1 and <fdatei>_dis2. For example, f6181_dis1 contains the first interval forecast data for June 18, 2002, at 20:00 using the first method (2,491 data points representing rainfall at each grid point for that interval), while f6183_dis2 contains the third interval forecast data for the same date using the second method.\nThe MEASURING folder contains 41 files named <date>.SIX. For instance, 020618.SIX represents the measured rainfall data at various stations for the four consecutive intervals starting at 21:00 on June 18, 2002. The data format of these files is as follows:\nStation ID Latitude Longitude Interval 1 Interval 2 Interval 3 Interval 4  \n58138 32.9833 118.5167 0.0000 0.2000 10.1000 3.1000  \n58139 33.3000 118.8500 0.0000 0.0000 4.6000 7.4000  \n58141 33.6667 119.2667 0.0000 0.0000 1.1000 1.4000  \n58143 33.8000 119.8000 0.0000 0.0000 0.0000 1.8000  \n58146 33.4833 119.8167 0.0000 0.0000 1.5000 1.9000  \n...  \n",
        "variable_description": [
            {}
        ],
        "addendum": "The meteorological department aims to establish a mathematical model and method for scientifically evaluating the effectiveness of rainfall forecasting methods. They have provided 41 days of forecast data from two different methods and the corresponding measured data. The forecast data is stored in the folder FORECAST, while the measured data is in the folder MEASURING. All files can be opened and read using the Windows 'WordPad' program.\nRainfall is measured in millimeters, with values less than 0.1 mm considered as no rain.",
        "data_num": 2,
        "source": "cumcm2005c",
        "source_type": "CUMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Hydrometeorological Verification Specialist",
                "details": "As a Hydrometeorological Verification Specialist, you possess expertise in statistical forecasting evaluation, spatial data alignment, and meteorological metrics. You evaluate the depth of analysis by examining whether the solution employs appropriate verification methods for both continuous (e.g., RMSE, MAE, correlation coefficients) and categorical (e.g., confusion matrix, precision, recall, CSI) rainfall data. You assess the rationality of reasoning in addressing spatial mismatches between gridded forecasts (53×47 grid points) and unevenly distributed observation stations—such as the use of spatial interpolation (e.g., kriging, inverse distance weighting) or nearest-neighbor matching to align forecast and measured data. Additionally, you verify if the integration of public perception (e.g., differential weighting of over/underestimation errors for high-impact categories like extreme rainstorms) is supported by logical, evidence-based justification, ensuring the evaluation framework reflects real-world decision-making needs."
            },
            {
                "name": "Forecast Innovation and User-Centric Evaluation Expert",
                "details": "As a Forecast Innovation and User-Centric Evaluation Expert, you specialize in identifying novel approaches to rainfall forecast evaluation that go beyond traditional metrics. You assess innovation by examining whether the solution introduces creative methods for integrating spatial variability (e.g., object-based verification to evaluate rainfall pattern similarity, not just point-wise errors) or dynamic weighting of performance based on rainfall severity (e.g., amplifying errors for extreme rainstorms critical to public safety). You also evaluate how public perception is innovatively quantified—such as the use of utility theory to model user dissatisfaction from misclassification (e.g., cost of false alarms vs. missed warnings) or crowdsourced perception data to calibrate category-specific error weights. Your expertise ensures the solution advances beyond standard verification by prioritizing forecast value to end-users, making the evaluation more actionable for public communication and decision support."
            }
        ]
    },
    {
        "id": 8,
        "title": "DVD Online Rental",
        "background": "With the advent of the information age, the internet has become an increasingly indispensable element in people's lives. Many websites leverage their extensive resources and popularity to offer increasingly specialized and convenient services to their members. For example, online rental of audiovisual products is a viable service. This service fully capitalizes on the many advantages of the internet, including broad dissemination, direct reach to core consumer groups, strong interactivity, high sensory appeal, and relatively low costs, to provide customers with more comprehensive services.\n\nConsider the following online DVD rental problem. Customers pay a monthly fee to become members and subscribe to the DVD rental service. Members can submit orders online for DVDs they are interested in, and the website will fulfill these orders as much as possible via courier. The orders submitted by members include multiple DVDs, ranked based on their preference. The website distributes DVDs based on its current inventory and the members' orders. Each member can rent DVDs no more than twice a month, with each rental consisting of three DVDs. After watching the three DVDs, members simply return them in envelopes provided by the website (postage paid by the website) to proceed with the next rental",
        "problem_requirement": "Please consider the following questions:\n\n1) The website is planning to purchase some new DVDs. Through a survey of 1,000 members, the number of members willing to watch these DVDs was obtained (Table 1 provides data for five such DVDs). Additionally, historical data shows that 60% of members rent DVDs twice a month, while the remaining 40% rent only once. Assuming the website currently has 100,000 members, for each DVD in Table 1, how many copies should be prepared at minimum to ensure that at least 50% of the members who want to watch the DVD can do so within one month? What if the requirement is to ensure that at least 95% of the members can watch the DVD within three months?\n\n2) Table 2 lists the current inventory of 20 types of DVDs held by the website and the online orders of 100 members currently needing processing. How should these DVDs be allocated to maximize member satisfaction? Please specify which DVDs the first 30 members (i.e., C0001 to C0030) will receive.\n\n3) Continuing with Table 2, assume that the current inventory of all DVDs listed in Table 2 is zero. If you are the website's management personnel, how would you determine the purchase quantity for each DVD and how to allocate them to ensure that 95% of the members can watch the DVDs they want within one month, while maximizing satisfaction?\n\nTable 1 Partial results of a survey of 1,000 members  \nDVD Name  DVD1  DVD2  DVD3  DVD4  DVD5  \nNumber of Members Willing to Watch  200  100  50  25  10  \n\nTable 2 Current DVD Inventory and Online Orders of Members (Example Format)  \nDVD Number  D001  D002  D003  D004  …  \nCurrent DVD Inventory  8  1  22  10  …  \n\nOnline Orders of Members  \n  C0001  0  0  2  0  …  \n  C0002  1  0  9  0  …  \n  C0003  0  6  0  0  …  \n  C0004  0  0  0  0  …  \n  …  …  …  …  …  …  \nNote: D001-D020 represent 20 types of DVDs, and C0001-C0100 represent 100 members. The numbers in the online orders (1, 2, …) indicate the members' preference levels, with smaller numbers indicating higher preference. The number 0 indicates that the corresponding DVD is not currently in the member's online order.  \n(Note: , )",
        "dataset_path": [
            "data1.xlsx"
        ],
        "dataset_description": "Table 2 lists the current inventory of 20 types of DVDs held by the website and the online orders of 100 members currently needing processing. The data for Table 2 is located in the file data1.xlsx, ",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 1,
        "source": "cumcm2005d",
        "source_type": "CUMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Operations Research and Inventory-Demand Modeling Expert",
                "details": "As an Operations Research and Inventory-Demand Modeling Expert, you specialize in quantitative analysis, demand forecasting, inventory optimization, and resource allocation under constraints. Your expertise spans statistical scaling, rental frequency modeling, and satisfaction rate calculation. You will evaluate the depth of analysis by verifying if the solution correctly scales member demand from the 1,000-member survey to 100,000 members, accurately incorporates rental frequency (60% renting twice/month, 40% once/month) into effective demand per member, and logically derives minimum DVD copies for 50% satisfaction in one month and 95% in three months. For allocation problems (Parts 2 and 3), you will assess the rationality of the optimization framework: whether preference levels (smaller numbers = higher priority) are appropriately weighted in the objective function, if inventory and per-member rental constraints (e.g., max 3 DVDs per rental) are strictly enforced, and if the allocation strategy (e.g., greedy, integer programming, or priority-based) aligns with maximizing member satisfaction. Your analysis ensures the solution’s reasoning is mathematically rigorous, data-driven, and operationally feasible."
            },
            {
                "name": "Innovation in Member-Centric Service Optimization Specialist",
                "details": "As an Innovation in Member-Centric Service Optimization Specialist, you focus on creative approaches to enhance satisfaction, demand responsiveness, and resource efficiency in service operations. Your expertise includes novel demand modeling, adaptive allocation strategies, and multi-objective optimization beyond traditional inventory methods. You will evaluate the innovation of solutions by checking if they go beyond basic deterministic demand scaling (Part 1) to incorporate probabilistic demand uncertainty (e.g., variability in member rental timing or return delays). For allocation (Parts 2 and 3), you look for innovations like dynamic preference weighting (e.g., considering member tenure or rental history to prioritize loyal customers), real-time allocation adjustments based on return predictions, or hybrid models that balance short-term satisfaction (current orders) with long-term retention (e.g., ensuring high-demand DVDs are available for future high-priority members). For Part 3, you assess if the purchase quantity determination integrates satisfaction elasticity (e.g., marginal gains from additional copies) or uses machine learning to predict latent demand (members who might order if stock is available). Your analysis highlights whether the solution introduces groundbreaking methods that improve member experience, operational agility, or cost-effectiveness compared to conventional inventory-allocation models."
            }
        ]
    },
    {
        "id": 9,
        "title": "Evaluation of AIDS Therapies and Prediction of Treatment Efficacy",
        "background": "AIDS is one of the most severe plagues in human society. In the over 20 years since its discovery in 1981, it has claimed nearly 30 million lives.\n\nThe medical full name of AIDS is \"Acquired Immune Deficiency Syndrome,\" abbreviated as AIDS in English. It is caused by the AIDS virus (medically known as \"Human Immunodeficiency Virus,\" abbreviated as HIV). This virus destroys the human immune system, rendering the body unable to resist various diseases, thereby severely endangering human life. The CD4 cells in the human immune system play a crucial role in defending against HIV invasion. When CD4 cells are infected by HIV and undergo lysis, their numbers decrease sharply, leading to a rapid increase in HIV and the onset of AIDS.\n\nThe goal of AIDS treatment is to minimize the amount of HIV in the human body while increasing the production of CD4 cells, or at least effectively slowing the rate of CD4 reduction to enhance immune capacity.\n\nTo date, no cure for AIDS has been found. Current AIDS treatments not only have side effects on the human body but are also costly. Many countries and medical organizations are actively experimenting and searching for better AIDS therapies.",
        "problem_requirement": "Please complete the following tasks:\n(1) Using the data from data1, predict the effects of continued treatment or determine the optimal time to terminate treatment (continued treatment refers to continuing medication after the end of testing; if continued medication is deemed ineffective, early termination may be chosen).\n(2) Using the data from data2, evaluate the advantages and disadvantages of the four therapies (based solely on CD4 levels), and for the superior therapy(s), predict the effects of continued treatment or determine the optimal time to terminate treatment.\n(3) The prices of AIDS medications provided by major suppliers to developing countries are as follows: 600mg zidovudine 1.60, 400mg didanosine 0.85, 2.25mg zalcitabine 1.85, and 400mg nevirapine 1.20. If patients need to consider the costs of the four therapies, how would this change the evaluation and prediction (or early termination) in (2)?",
        "dataset_path": [
            "data1.txt",
            "data2.txt"
        ],
        "dataset_description": "Two sets of data published by the American AIDS Clinical Trials Group (ACTG) have been obtained. ACTG320 (see data1.txt) includes data from over 300 patients who were simultaneously taking three drugs—zidovudine, lamivudine, and indinavir—with their CD4 and HIV concentrations (number per milliliter of blood) tested every few weeks. 193A (see data2.txt) involves over 1,300 patients randomly divided into four groups, each receiving one of the following four therapies, with CD4 concentrations tested approximately every eight weeks (this dataset lacks HIV concentration data due to the high cost of testing). The daily medications for the four therapies are as follows: 600mg zidovudine or 400mg didanosine, with these two drugs alternated monthly; 600mg zidovudine plus 2.25mg zalcitabine; 600mg zidovudine plus 400mg didanosine; and 600mg zidovudine plus 400mg didanosine, plus 400mg nevirapine.",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 2,
        "source": "cumcm2006b",
        "source_type": "CUMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Biostatistical and Clinical Efficacy Evaluator",
                "details": "As a Biostatistical and Clinical Efficacy Evaluator, you specialize in longitudinal clinical data analysis, CD4 cell dynamics, and the statistical validation of treatment efficacy endpoints. Your expertise includes time-series modeling (e.g., linear mixed-effects models, growth curve analysis), survival analysis for treatment termination decisions, and clinical relevance assessment of immune markers (e.g., CD4 level trends as a proxy for immune function). You will evaluate whether the solution thoroughly models CD4 trajectories over time, accounts for inter-patient variability and missing data, uses appropriate statistical tests to distinguish therapy effects, and ensures that reasoning for optimal termination (e.g., CD4 decline thresholds, non-inferiority margins) is methodologically rigorous and clinically actionable."
            },
            {
                "name": "Innovative Pharmacoeconomic and Predictive Analytics Specialist",
                "details": "As an Innovative Pharmacoeconomic and Predictive Analytics Specialist, you combine expertise in advanced predictive modeling, pharmacoeconomic innovation, and clinical trial design. Your knowledge spans novel statistical techniques (e.g., Bayesian forecasting, machine learning for time-series prediction), cost-effectiveness frameworks beyond traditional ICERs, and creative integration of efficacy and cost data. You will assess whether the solution introduces innovative methods for predicting treatment outcomes (e.g., hybrid models merging CD4 trends with patient covariates), develops dynamic cost-efficacy metrics (e.g., marginal cost per CD4 cell gain), or applies adaptive algorithms for real-time termination decisions. Your focus is on identifying breakthrough approaches that enhance precision in therapy evaluation and prediction."
            }
        ]
    },
    {
        "id": 10,
        "title": "How much discount does the mobile 'plan' offer?",
        "background": "Mobile phones have become essential tools in people's daily work, social interactions, business operations, and other social activities. In recent years, the volume of communication services has grown rapidly.  \nThe issue of mobile phone pricing has always been a hot topic of public concern. For many years, pricing plans have seen no substantial changes. However, since January 2007, major operators such as China Mobile and China Unicom in Shanghai, Beijing, Guangdong, and other regions have successively launched \"one-way charging plans\" for mobile phones—various branded \"packages.\" The variety of mobile phone \"packages\" is dazzling, leaving consumers bewildered. People can't help but wonder: How much discount do these mobile \"packages\" actually offer?",
        "problem_requirement": "Use mathematical modeling to analyze and study the following questions:  \n\n(1) Provide the pricing calculation methods for each \"package\" plan in Beijing and Shanghai. For users with different (call volume) needs, analyze and explain which types of users each \"package\" plan is suitable for.  \n\n(2) Propose your criteria and methods for evaluating various pricing plans. Based on this, analyze and compare the \"package\" plans launched in Beijing and Shanghai with the current pricing standards, and provide an evaluation.  \n\n(3) On May 23, 2007, Beijing Mobile launched the so-called \"Called Party Free Plan\" for Global Connect users. This plan includes a monthly fee of 50 yuan, free local incoming calls, and all other pricing items aligned with the current pricing standards. It also requires users to stay subscribed for at least one year. How would you evaluate this plan? Explain your reasoning.  \n\n(4) If China Mobile hires you to help design a pricing plan for Global Connect mobile phones, what factors would you consider? Based on your research findings and the actual situations in Beijing and Shanghai, and under the condition that the operator’s revenue does not decrease by more than 10% compared to the existing \"package\" plans, use mathematical modeling methods to design a \"package\" plan that you consider reasonable.",
        "dataset_path": [
            "data1.txt",
            "data2.txt"
        ],
        "dataset_description": "data1: the volume of communication services; data2: he current pricing standards of China Mobile and the \"Enjoy 99 Package\" for China Mobile's Global Connect in Beijing, as well as the \"Global Connect 68 Package\" in Shanghai.",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 2,
        "source": "cumcm2007c",
        "source_type": "CUMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Telecom Pricing Strategy and Quantitative Analysis Expert",
                "details": "As a Telecom Pricing Strategy and Quantitative Analysis Expert, you specialize in mathematical modeling of pricing plans, consumer cost-benefit analysis, and data-driven evaluation of telecom services. Your expertise includes interpreting pricing data (e.g., from data2 on current standards and packages), segmenting users by usage patterns (using data1 on communication volume), and validating the rigor of quantitative models. You will evaluate whether the solution: 1) Accurately derives pricing calculation methods for Beijing (e.g., 'Enjoy 99 Package') and Shanghai (e.g., 'Global Connect 68 Package') by correctly parsing data2; 2) Logically segments users by call volume (e.g., low/medium/high) using statistical or break-even analysis to determine package suitability; 3) Develops comprehensive evaluation criteria (e.g., total cost for varying usage, transparency, consumer surplus) grounded in economic principles; 4) Rigorously assesses the 'Called Party Free Plan' by comparing costs for users with different incoming/outgoing ratios and analyzing revenue impacts of the 1-year lock-in; 5) Ensures the new plan design (part 4) adheres to the 10% revenue constraint through valid optimization models (e.g., linear programming, regression) and justifies key assumptions (e.g., cost structure, user demand elasticity). Your focus is on verifying that the analysis is mathematically sound, data-driven, and logically consistent from assumptions to conclusions."
            },
            {
                "name": "Telecom Innovation and Consumer-Centric Design Specialist",
                "details": "As a Telecom Innovation and Consumer-Centric Design Specialist, you focus on identifying novel approaches to pricing model design, user-centric evaluation, and adaptive service strategies. Your expertise spans behavioral economics, innovative pricing mechanisms, and leveraging communication volume data (data1) to anticipate user needs. You will evaluate whether the solution: 1) Introduces innovative user segmentation beyond basic call volume (e.g., incorporating time-of-use patterns, incoming/outgoing call ratios, or predictive usage trends from data1); 2) Develops creative evaluation criteria (e.g., flexibility of plans, customization options, or 'pain points' addressed like lock-in period mitigation); 3) Proposes unique insights for the 'Called Party Free Plan' (e.g., analyzing network effects of free incoming calls or behavioral incentives from the 1-year subscription); 4) Designs a forward-thinking new plan (part 4) that goes beyond static packages—e.g., tiered pricing with adjustable thresholds, usage-based discounts, or bundling with underutilized services (if supported by data); 5) Integrates consumer surplus or satisfaction metrics into the revenue-constrained optimization, ensuring the plan balances operator interests with user value. Your focus is on assessing whether the solution breaks from traditional telecom pricing paradigms and offers actionable, innovative strategies that align with evolving user behaviors."
            }
        ]
    },
    {
        "id": 11,
        "title": "Analysis of Lithium Battery Reliability Under Accelerated Degradation Conditions",
        "background": "In the current global energy transformation, new energy, as a substitute for petroleum-based energy sources, holds strategic importance in addressing climate crisis, carbon neutrality, and energy regeneration. As one of the new energy technologies, lithium battery technology has drawn industry attention to the reliability of battery products. Currently, factors considered to affect the lifespan and reliability of lithium batteries include: the number of charge-discharge cycles, temperature, depth of discharge, and different materials.",
        "problem_requirement": "Based on the dataset provided in 'data1' and 'data2' and following the requirements for statistical case study writing, please address the following issues: \n1. Based on the given information and data, including different battery components, four different accelerated temperature settings, 2,000 charge-discharge cycles, and the corresponding recorded remaining capacity of the battery cells, establish a nonlinear parametric statistical analysis model and perform parameter estimation. Predict the point estimate and interval estimate of the time when 50% of the battery products fail under high-temperature conditions of 50°C (a product is considered failed when its remaining capacity is less than 75%). \n2. Based on the failure cycle counts of three different materials after charge-discharge cycles (data2), determine which material has better reliability and provide specific reasons.",
        "dataset_path": [
            "data1.xlsx",
            "data2.xlsx"
        ],
        "dataset_description": "",
        "variable_description": [
            {
                "barcode": "Component Serial Number",
                "Temperature": "Component Test Temperature",
                "Cycle_No.": "Number of Charge-Discharge Cycles",
                "Capacity retention": "Remaining Capacity"
            },
            {
                "Group": "Material Type",
                "Number of Charge": "Discharge Cycles When Product Remaining Capacity Drops Below 75%"
            }
        ],
        "addendum": "",
        "data_num": 1,
        "source": "MAS2022a",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Reliability Engineering Statistician (Accelerated Degradation Focus)",
                "details": "As a Reliability Engineering Statistician specializing in accelerated degradation testing (ADT), you possess deep expertise in nonlinear parametric statistical modeling, accelerated life testing (ALT) methodologies, and degradation data analysis. Your role is to evaluate the depth of analysis and rationality of reasoning in the solution by assessing: (1) the appropriateness of the selected nonlinear degradation model (e.g., Wiener process, gamma process, or empirical models like the Arrhenius-based exponential model) for capturing capacity retention trends under varying temperatures and cycles; (2) the validity of parameter estimation methods (e.g., maximum likelihood estimation, Bayesian inference) and whether they account for key variables (temperature, component variability via 'barcode'); (3) the rigor in extrapolating to 50°C (e.g., correct application of acceleration models like Arrhenius to link temperature to degradation rate); (4) the integration of the 75% capacity failure threshold into time-to-failure predictions; and (5) the justification for point/interval estimate methods (e.g., bootstrap, delta method) and their alignment with statistical uncertainty principles. You ensure the reasoning logically connects data, model, and reliability predictions."
            },
            {
                "name": "Innovative Battery Reliability Modeling Specialist",
                "details": "As an Innovative Battery Reliability Modeling Specialist, you focus on identifying novel approaches and methodological advancements in degradation analysis and material comparison. Your expertise spans statistical innovation, multi-stress factor integration, and modern reliability metrics. You evaluate the solution’s innovation by assessing: (1) whether it integrates physics-informed elements (e.g., electrochemical degradation mechanisms) with statistical models to enhance predictive power; (2) the use of advanced techniques to account for component variability (e.g., hierarchical Bayesian models with random effects for 'barcode' serial numbers); (3) innovations in temperature acceleration modeling (e.g., non-Arrhenius temperature dependencies or interaction terms between temperature and cycles); (4) novel material reliability comparison methods beyond mean failure cycles (e.g., survival analysis with accelerated failure time models, quantile regression for variability assessment, or degradation path similarity metrics); and (5) the adoption of uncertainty quantification advancements (e.g., Bayesian updating with prior electrochemical knowledge or adaptive sampling for interval estimates). You highlight how the solution pushes beyond traditional ADT frameworks to address real-world battery reliability challenges."
            }
        ]
    },
    {
        "id": 12,
        "title": "Statistical Analysis of Single-Cell RNA Sequencing Data for Cell Classification and Identification of Differentially Expressed Genes",
        "background": "Statistical Analysis of Single-Cell RNA Sequencing Data for Cell Classification and Identification of Differentially Expressed Genes",
        "question": "Single-cell RNA sequencing enables geneticists to better explore gene expression profiles at the single-cell level, understand differences in cell expression and function across various tissue microenvironments, and provides unprecedented opportunities for discovering rare cell types and tracing cell lineage differentiation trajectories. Currently, single-cell RNA sequencing has become the preferred method for investigating key biological questions related to cellular heterogeneity. While advancements in this technology have generated vast amounts of data for single-cell RNA analysis, they have also introduced challenges in data processing, including how to accurately classify cells based on sequencing information and identify differentially expressed genes.",
        "problem_requirement": "Based on the dataset provided in 'Attachment 2', and following the requirements for statistical case study writing, address the following tasks: \n1. Clean and preprocess the data, and use analytical methods (such as Sparse SVD, Clustering, etc.) to classify the collected cells.\n2. After cell classification, apply statistical analysis methods (such as Mixed Model ANOVA) to identify significantly differentially expressed genes.",
        "dataset_path": [
            "barcodes.tsv",
            "genes.tsv",
            "matrix.mtx"
        ],
        "dataset_description": "barcodes.tsv: Single-cell RNA (scRNA) gene barcode information; Genes.tsv: Cell information; Matrix.mtx: {Column 1: Cell row number Column 2: Gene row number (corresponding gene information is in barcodes.tsv) Column 3: Gene counts",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 3,
        "source": "MAS2022b",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Single-Cell RNA Sequencing Data Analysis Methodologist",
                "details": "As a Single-Cell RNA Sequencing Data Analysis Methodologist, you specialize in the technical and statistical rigor of scRNA-seq data processing pipelines, including preprocessing (normalization, batch effect correction, handling sparsity and dropout events), cell classification (dimensionality reduction via Sparse SVD, clustering algorithms like t-SNE/UMAP, k-means, or Leiden clustering), and differential gene expression (DEG) analysis (Mixed Model ANOVA, model assumption validation, multiple testing correction). Your expertise includes evaluating the depth of analysis by assessing whether each step is thoroughly justified (e.g., rationale for choosing Sparse SVD over PCA, clustering validation metrics such as silhouette scores or adjusted Rand index), the rationality of reasoning by checking if statistical methods align with data characteristics (e.g., Mixed Model ANOVA appropriately accounting for random effects of cellular heterogeneity), and if biological interpretability is integrated (e.g., cluster annotations validated against known cell type markers). You ensure the solution avoids technical pitfalls (e.g., overcorrection of batch effects, ignoring sparsity in count data) and that conclusions are robustly supported by statistical evidence."
            },
            {
                "name": "Computational Innovation in Single-Cell Genomics Expert",
                "details": "As a Computational Innovation in Single-Cell Genomics Expert, you focus on advancing scRNA-seq analytical pipelines through novel method development or creative adaptation of existing tools. Your expertise spans emerging techniques in preprocessing (e.g., deep learning-based imputation for dropout mitigation, multi-modal integration with spatial or protein data), innovative dimensionality reduction (e.g., graph neural network-based SVD variants, contrastive learning for feature extraction), cutting-edge clustering (e.g., trajectory-aware clustering to capture differentiation states, rare cell type detection via adaptive thresholding), and DEG analysis (e.g., sc-specific models like MAST or DESeq2 adapted for single-cell variability, or integrating prior biological networks to prioritize DEGs). You evaluate innovation by assessing if the solution introduces unique approaches (e.g., combining Sparse SVD with attention mechanisms to highlight biologically relevant genes) addresses unmet challenges (e.g., improving cluster resolution for closely related cell subtypes), or bridges technical and biological gaps (e.g., using transfer learning from bulk RNA-seq to enhance scRNA-seq classification), thereby pushing beyond standard pipelines to generate novel biological insights."
            }
        ]
    },
    {
        "id": 13,
        "title": "E-Commerce Data Analysis: Product Valuation, High-Value Customer Identification, and Recommendation System",
        "background": "With the continuous development of major e-commerce platforms, industry competition has become increasingly fierce. Analyzing platform order data can enhance customer loyalty, stabilize customer traffic, and continuously adjust operational strategies. This type of analysis often focuses on addressing the following questions: How do sales performance changes compare to the previous quarter/year, and if there is a decline, what are the reasons? What are the characteristics of user consumption behavior, and is there discernible patterns in high-net-worth customers' behavior? How can personalized sales predictions be made based on the relationship between 'products & customers' to develop recommendation systems, etc.?",
        "problem_requirement": "Based on the dataset provided in 'ecommercedata-Historical_Orders.xlsx' and 'ecommercedata-Forecast.xlsx', and following the case study requirements, address the following issues:  \n1. Regarding product sales performance, design a data analysis and feature extraction process. Use clustering methods (such as k-means) to evaluate the commercial value of products in terms of 'whether they are best-sellers', 'whether they are highly profitable', 'whether they have high return rates', and other aspects. \n2. According to the Pareto Principle, i.e., 20% of customers purchase 80% of the products/contribute 80% of the revenue. In businesses, these 20% of customers, who drive 80% of the company's revenue, are often referred to as 'high-value customers'. Design an analysis process and method to identify high-value customers based on customer retention (measured by, but not limited to, order time intervals and active duration), consumption amount, and return rates.  \n3. The 'recommendation system' commonly used in e-commerce platforms is a set of algorithms that recommend items to users based on their purchase (/collection/browsing/...) history. The algorithm logic is as follows: Based on a customer’s purchase history, find the N most similar customers and calculate similarity scores based on their purchase records. The M products with the highest scores will be recommended to the original customer. The data file 'ecommercedata-prediction.xlsx' in Attachment 3 contains customers who placed orders in the 13th month. Based on their purchase history from the previous year, predict the products they are likely to purchase in that month and evaluate the accuracy of the algorithm.",
        "dataset_path": [
            "ecommercedata-Historical_Orders.xlsx",
            "ecommercedata-Forecast.xlsx"
        ],
        "dataset_description": "ecommercedata-Historical_Orders.xlsx: This file contains the platform's historical orders over one year. It is used as the original data for creating the training set; ecommercedata-Forecast.xlsx: This file contains the Customer IDs of customers who placed orders on the platform in the 13th month. It is used for sales forecasting.",
        "variable_description": [
            {
                "InvoiceNo": "Invoice number, a unique 6-digit integer assigned to each transaction. Codes for return orders start with the letter 'c'.",
                "StockCode": "Product code, a unique 5-digit integer assigned to each distinct product.",
                "Description": "Product description.",
                "Quantity": "The quantity of each product in each transaction.",
                "InvoiceDate": "Transaction date and time.",
                "UnitPrice": "Unit price (in GBP).",
                "CustomerID": "Customer ID.",
                "Country": "The country/region where the customer is located."
            }
        ],
        "addendum": "",
        "data_num": 2,
        "source": "MAS2022c",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "E-Commerce Data Analysis Methodologist",
                "details": "As an E-Commerce Data Analysis Methodologist, you specialize in evaluating the rigor of analytical workflows, feature engineering logic, and the appropriateness of statistical/machine learning methods for business objectives. You will assess whether the solution adequately addresses product valuation (e.g., if 'best-sellers', 'highly profitable', and 'high return rates' are operationalized with meaningful metrics like sales volume, gross margin, and return rate ratios), whether clustering methods (k-means) are properly validated (e.g., using elbow method or silhouette scores), and if customer retention metrics (order intervals, active duration) are logically integrated with consumption amount and return rates for high-value customer identification. Your expertise ensures the reasoning connects technical steps (e.g., feature scaling for clustering) to business interpretability (e.g., actionable product segments) and that the recommendation system’s similarity calculation (e.g., cosine similarity on purchase vectors) is methodologically sound."
            },
            {
                "name": "E-Commerce Analytics Innovation Strategist",
                "details": "As an E-Commerce Analytics Innovation Strategist, you focus on identifying novel approaches and creative problem-solving in data analysis. You will evaluate if the solution goes beyond conventional methods: for product valuation, does it incorporate dynamic features (e.g., seasonal sales trends, cross-product co-purchase patterns) instead of static metrics? For high-value customer identification, does it extend the Pareto Principle with predictive elements (e.g., lifetime value forecasting using machine learning) rather than just historical revenue? For the recommendation system, does it use hybrid techniques (e.g., combining collaborative filtering with content-based features like product category or country) or advanced similarity measures (e.g., using matrix factorization to handle sparse purchase data)? Your assessment highlights innovations that enhance business impact, such as 'profitability-return tradeoff scores' for products or 'retention risk-adjusted value' for customers."
            }
        ]
    },
    {
        "id": 14,
        "title": "Deep Learning-Based Quantitative Trading: Stock Selection and Return Prediction Using Market Data",
        "background": "In the field of quantitative trading, predicting future security returns is a core objective of the industry. There are two primary distinct approaches in trading: fundamental analysis and quantitative trading. Fundamental analysis involves making trading decisions based on subjective views about the future direction of industries or companies, relying mainly on public information such as market news, company statistics, and a range of financial statement releases. On the other hand, quantitative trading strategies use mathematical models to make decisions, thereby avoiding subjective judgments and emotional biases. Traditional quantitative strategy methods include the use of statistical models such as linear regression, ARIMA models, and GARCH models to capture the characteristics of time series and the randomness of volatility. While these methods have proven effective for a certain period, they have become insufficient over time with the rapid development of deep learning. How to use deep learning models to predict stock returns has become an urgently needed research topic.",
        "problem_requirement": "Based on the daily fluctuation data of nearly 5,000 stocks from 2020 to 2022, and following the requirements of statistical case studies, analyze past stock price trends and address the following issues:  \n1. Use quantitative methods to select suitable stocks or stock portfolios, establish a statistical quantitative trading model, and formulate a definitive quantitative trading strategy. \n2. Using a base capital of 100,000 RMB and data from after June 2022 as the test set, calculate the total return rate obtained by employing the quantitative stock selection method and quantitative trading strategy.",
        "dataset_path": [
            "2020",
            "2021",
            "2022"
        ],
        "dataset_description": "Daily fluctuation data of nearly 5,000 stocks, with the time range covering from 2020 to 2022.",
        "variable_description": [
            {
                "code": "Stock Code",
                "name": "Stock Name",
                "rise": "Price Increase Rate",
                "zhenfu": "Price Fluctuation Range",
                "lastclose": "Previous Closing Price",
                "open": "Opening Price",
                "close": "Closing Price",
                "high": "Daily High Price",
                "low": "Daily Low Price",
                "avg": "Average Price",
                "chengjiaogushu": "Trading Volume (Shares)",
                "huanshoulv": "Turnover Rate",
                "chengjiaoe": "Trading Amount",
                "zongshizhi": "Total Market Capitalization",
                "ltshizhi": "Circulating Market Capitalization",
                "zongguben": "Total Share Capital",
                "ltguben": "Circulating Share Capital",
                "shiyinglv": "Price-Earnings Ratio (P/E Ratio)",
                "shijinglv": "Price-to-Book Ratio (P/B Ratio)"
            }
        ],
        "addendum": "",
        "data_num": 3,
        "source": "MAS2023a",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Quantitative Trading Strategy Validation Expert",
                "details": "As a Quantitative Trading Strategy Validation Expert, you specialize in evaluating the rigor of statistical modeling, stock selection logic, and trading strategy formulation in quantitative finance. Your expertise spans financial time series analysis, risk-adjusted return assessment, and backtesting methodology. You will assess the depth of analysis by examining whether the solution properly handles critical factors such as feature engineering (e.g., relevance of 'rise', 'zhenfu', 'huanshoulv' in predicting returns), model validation techniques (e.g., out-of-sample testing, cross-validation across market regimes), and the rationality of trading rules (e.g., entry/exit criteria, position sizing, and mitigation of look-ahead/selection bias). Additionally, you will verify if the return calculation accounts for real-world constraints like transaction costs, slippage, and liquidity, ensuring the strategy’s reasoning aligns with practical quantitative trading principles."
            },
            {
                "name": "Deep Learning for Financial Forecasting Innovation Expert",
                "details": "As a Deep Learning for Financial Forecasting Innovation Expert, you are well-versed in cutting-edge deep learning architectures (e.g., transformers, graph neural networks, attention mechanisms) and their application to non-stationary, noisy financial data. Your focus is on evaluating how the solution advances beyond traditional statistical models (ARIMA, GARCH) and conventional deep learning approaches (LSTMs). You will assess innovation by examining novel model designs (e.g., multi-scale temporal modeling for daily 'rise' and 'zhenfu' data, cross-stock relational learning using 'zongshizhi' or 'shiyinglv'), integration of alternative data modalities (if any), or adaptive mechanisms to address financial market dynamics (e.g., regime-switching layers, uncertainty quantification). Additionally, you will evaluate whether the solution introduces innovative risk-control features within the deep learning framework (e.g., end-to-end reward optimization with Sharpe ratio as a loss term) or novel stock selection paradigms (e.g., dynamic portfolio embedding) that enhance predictive power or strategy robustness."
            }
        ]
    },
    {
        "id": 15,
        "title": "Nanning Citrus A Industry Index",
        "background": "Nanning City, located in the subtropical region of Guangxi, enjoys abundant sunlight, plentiful rainfall, and suitable temperatures, providing ideal conditions for the growth of many cash crops. Citrus A retains its fruit well in winter with minimal drop, has strong fruit-setting ability, and a long harvesting period. It can be marketed from January to February each year, with the latest harvest extending until May. Since it primarily hits the market around March, it avoids peak seasons of other varieties, resulting in favorable sales and prices. However, the Citrus A industry in Nanning still faces several challenges, such as an underdeveloped agricultural technology innovation system, fragmented cultivation practices, weak infrastructure, and outdated commercialization processes. Therefore, compiling a Citrus A Industry Index for Nanning will help monitor the industry’s operational status in a timely manner, providing a scientific basis for the government and businesses to assess economic conditions from operational and consumer market perspectives.",
        "problem_requirement": "Based on the provided 'data1.xlsx', and following statistical case study writing requirements, address the following tasks:\n1. Identify the factors affecting the yield of Citrus A, construct a hierarchical model of Citrus A yield, dynamically track the yield index during the reporting period, display changes and trends in yield indices (including total output and yield per mu), and analyze the main reasons for changes in the yield index.\n2. Identify the factors affecting the sales price of Citrus A, construct a hierarchical model of the sales price structure, dynamically track the price index and changes in sub-indices during the reporting period, display the magnitude and trends of price index changes, and analyze the main reasons for price fluctuations.\n3. Identify key factors in the production and sales chain of the Citrus A industry, clarify the interrelationships and interactions among these factors, and construct a Citrus A Industry Index to provide users with a multi-dimensional overview of the industry’s operational status.",
        "dataset_path": [
            "data1.xlsx"
        ],
        "dataset_description": "",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 1,
        "source": "MAS2023b",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Citrus A Yield-Price Factor Analysis & Hierarchical Modeling Expert",
                "details": "As an expert specializing in agricultural economics, agronomic systems, and hierarchical statistical modeling, you will evaluate the depth and rationality of solutions. Your focus includes: (1) Assessing the comprehensiveness of identified yield/price factors (e.g., whether natural factors like temperature/precipitation, human factors like cultivation technology/fertilizer use, and market factors like supply-demand dynamics are fully covered). (2) Scrutinizing the logical rigor of hierarchical models—do layers (e.g., macro/meso/micro for yield; production/market/consumer for price) reflect clear causal relationships, and are sub-factors within layers non-overlapping and hierarchically coherent? (3) Evaluating the statistical soundness of index tracking (e.g., appropriateness of base period selection, weighting methods, and temporal granularity) and whether trend analysis links index changes to specific drivers (e.g., correlating yield index drops with frost events via meteorological data). (4) Validating reasoning validity—are conclusions about 'main reasons' supported by quantitative evidence (e.g., regression analysis showing 70% of price fluctuation is explained by post-harvest logistics costs) rather than anecdotal claims."
            },
            {
                "name": "Agricultural Industry Index Innovation & Cross-Chain Integration Specialist",
                "details": "As an expert in agricultural innovation and multi-dimensional industry index design, you will assess the innovativeness of solutions. Your focus includes: (1) Evaluating novel factor integration—whether the solution goes beyond traditional yield/price drivers to include emerging elements (e.g., digital agriculture adoption rate, consumer preference shifts for 'late-market' citrus, or carbon footprint metrics). (2) Methodological innovation—does the Industry Index use dynamic weighting (e.g., machine learning to adjust factor weights based on real-time production-sales data) instead of static weights, or integrate cross-chain interactions (e.g., modeling how post-harvest storage technology affects both yield stability and price premium)? (3) Practical value innovation—does the index provide multi-dimensional insights beyond basic monitoring (e.g., predictive modules for price volatility, or 'vulnerability scores' for key chain links like transportation infrastructure)? (4) Interdisciplinary integration—whether the solution bridges agronomy, consumer behavior, and data science (e.g., using IoT sensor data for real-time yield potential updates or social media sentiment analysis for price trend forecasting)."
            }
        ]
    },
    {
        "id": 16,
        "title": "Industrial Chain Competitiveness Diagnostic and Enhancement Recommendation Model",
        "background": "An industrial chain refers to a chain-network-like industrial economic linkage formed within a specific region, based on division of labor and collaboration, connected by industrial ties, and centered on enterprises. Currently, global industrial chains are undergoing accelerated restructuring towards regionalization, localization, and lean procurement. In this context, many regions have proposed the implementation of a 'Chain Leader System' as a key measure to enhance the competitiveness of local key industrial chains. Among these efforts, diagnosing existing industrial chains, clarifying their competitiveness, and subsequently conducting 'chain complementing', 'chain strengthening', 'chain solidifying', and 'chain extending' are of significant importance for further investment promotion and coordinated allocation of various resources.",
        "problem_requirement": "Based on the dataset provided in 'data1.xlsx', and in accordance with statistical case study requirements, address the following issues:  \n1. Using the given industrial chain information and enterprise data, establish a diagnostic model to evaluate and assess the competitiveness of regional industrial chains, identifying shortcomings, strengths, and gaps.  \n2. Based on the diagnostic results, design a recommendation model and method for introducing enterprises targeted at 'chain complementing', 'chain strengthening', 'chain solidifying', and 'chain extending'.",
        "dataset_path": [
            "data1.xlsx"
        ],
        "dataset_description": "data1 contains enterprise data and is the basis for conducting regional industrial chain competitiveness diagnosis and targeted enterprise introduction recommendation model design.",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 1,
        "source": "MAS2023c",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Industrial Chain Diagnostic & Reasoning Expert",
                "details": "As an Industrial Chain Diagnostic & Reasoning Expert, you possess deep expertise in industrial economics, statistical modeling, and industrial chain dynamics. Your knowledge spans upstream-downstream linkage analysis, enterprise competitiveness metrics (e.g., R&D intensity, market share, scale), and regional industrial ecosystem evaluation. You will assess the depth of analysis by examining whether the diagnostic model comprehensively covers core dimensions (e.g., industrial completeness, technological innovation capability, resource allocation efficiency, and risk resilience) using data from 'data1.xlsx'. You will verify the rationality of reasoning by evaluating if the selection of indicators (e.g., number of key enterprises, value-added rate, patent density) is theoretically grounded and data-driven, if the identification of strengths/weaknesses/gaps is supported by statistical evidence from enterprise data, and if the logical chain between diagnostic results and recommendation objectives ('chain complementing', etc.) is rigorous and non-contradictory."
            },
            {
                "name": "Industrial Strategy Innovation Specialist",
                "details": "As an Industrial Strategy Innovation Specialist, you are well-versed in cutting-edge trends in industrial chain management, innovative analytical methodologies (e.g., network science, machine learning, predictive analytics), and global best practices in industrial upgrading. Your expertise includes exploring novel approaches to competitiveness diagnosis and recommendation design beyond traditional frameworks. You will evaluate innovation by assessing whether the diagnostic model integrates emerging dimensions (e.g., digitalization level, green development potential, cross-chain synergy) or leverages advanced tools (e.g., enterprise network graph analysis, spatial correlation modeling) using 'data1.xlsx'. For recommendations, you will check if the model adopts innovative strategies—such as AI-driven enterprise matching algorithms for 'chain extending', scenario simulation for future-proofing 'chain solidifying', or platform-based collaboration mechanisms for 'chain strengthening'—that go beyond conventional investment promotion lists, ensuring adaptability to regionalization/localization trends in global industrial chains."
            }
        ]
    },
    {
        "id": 17,
        "title": "Population Analysis and Urban Development Planning Based on Signaling Data ",
        "background": "With the acceleration of urbanization, urban population scales continue to expand, posing significant challenges to urban planning and management. Signaling data, as a critical component of mobile communication networks, contains vast amounts of user location information, providing robust data support for urban population analysis and planning. Through in-depth mining and analysis of signaling data, it is possible to understand the distribution, mobility patterns, and spatial characteristics of urban populations, thereby offering decision-making support for urban development planning.",
        "problem_requirement": "Based on the provided signaling dataset (Xinlin.xls), and following the requirements of statistical case study writing, complete the following tasks:  \n1. Analyze the population distribution and density across various urban areas using location information from the signaling data. Visualize the population distribution through heat maps or density maps, and identify densely and sparsely populated areas.  \n2. Reveal urban population mobility patterns across different time periods through time-series analysis of signaling data. This includes, but is not limited to, commuting patterns, holiday travel characteristics, and tourist hotspots.  \n3. Use statistical methods and machine learning algorithms to extract spatial features of the population from signaling data, such as residential areas, commercial districts, and transportation hubs. Analyze the impact of these features on urban planning and propose corresponding optimization suggestions.  \n4. Based on the above analysis results and the practical needs of urban development planning, propose targeted planning recommendations. These may include, but are not limited to, optimizing population distribution, arranging transportation facilities, and configuring public service facilities.  \n5. Additionally, search for relevant population data and actual urban development planning needs of the city where your participating institution is located (or a nearby city). Referring to the above analysis methods, propose targeted urban development planning recommendations.",
        "dataset_path": [
            "xinlin.csv"
        ],
        "dataset_description": "The provided dataset is the signaling data of a specific area in a city, which includes fields such as user IMSI, base station LACCI, base station longitude and latitude, start time, and end time.",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 1,
        "source": "MAS2024a",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Spatial-Temporal Data Analytics and Urban Planning Methodologist",
                "details": "As a Spatial-Temporal Data Analytics and Urban Planning Methodologist, you possess expertise in spatial statistics, time-series modeling, machine learning validation, and evidence-based urban planning. Your role is to evaluate the depth of analysis by assessing whether the solution comprehensively addresses population distribution/density (e.g., spatial interpolation methods for heatmaps, consideration of base station coverage biases), mobility patterns (e.g., granularity of time periods analyzed, segmentation of commuting vs. non-commuting flows), and spatial feature extraction (e.g., appropriateness of ML algorithms like clustering or classification for identifying residential/commercial hubs). You will verify the rationality of reasoning by checking if data insights (e.g., peak commuting hours, high-density clusters) logically lead to planning recommendations (e.g., transport network optimization, public service placement) and if statistical/machine learning results are validated (e.g., cross-validation for spatial feature models, significance testing for mobility pattern differences)."
            },
            {
                "name": "Urban Big Data Innovation and Smart Planning Strategist",
                "details": "As an Urban Big Data Innovation and Smart Planning Strategist, you specialize in emerging technologies for urban analytics, innovative use of signaling and multi-source data, and cutting-edge urban planning frameworks. Your role is to evaluate the innovation of the solution by assessing whether it goes beyond conventional approaches. This includes checking for novel methods in signaling data processing (e.g., real-time mobility inference from base station handover patterns, bias correction for non-uniform user representation), integration of multi-source data (e.g., combining signaling with POI, traffic, or social media data for richer spatial features), advanced visualization (e.g., interactive 4D density maps showing temporal-spatial dynamics), or innovative machine learning applications (e.g., deep learning for predicting population flow under future urban development scenarios). You will also assess if the solution introduces new planning paradigms (e.g., dynamic public service allocation based on real-time population hotspots) or actionable insights that traditional census/survey data cannot provide, enhancing the intelligence and adaptability of urban development planning."
            }
        ]
    },
    {
        "id": 18,
        "title": "Analysis of Influencing Factors and Multimodal Risk Assessment Model for PTSD in Rescue Personnel",
        "background": "Rescue personnel perform multiple tasks such as search and rescue, maintaining stability, and counterterrorism. They face high risks and heavy burdens related to post-traumatic stress disorder (PTSD). Early identification and clarification of the influencing factors of PTSD, along with establishing an effective prediction model and assessment tools for rescue personnel, are of positive significance in mitigating PTSD risks.",
        "problem_requirement": "Based on the provided dataset (data1.xlsx) and additional data (A1.csv - A25.csv), please address the following issues in accordance with statistical case study writing requirements:\n1. Analysis of Factors Influencing the Onset of PTSD in Rescue Personnel:     \na) Analysis of Protective Factors: Detailed analysis of the mechanisms by which factors that may have a protective effect against PTSD in rescue personnel operate.     \nb) Analysis of Risk Factors: Focused analysis of factors that may pose a risk for PTSD in rescue personnel, delving into the potential risk mechanisms that increase the likelihood of developing PTSD.     \nc) Analysis of Interaction Effects: Examination of potential interactions between factors, along with rational suggestions for preventive measures.\n2. Design of a PTSD Risk Assessment Model in a Multimodal Data Scenario:     \nWith the increasing application of information technology in clinical data research, the use of multimodal data, including unstructured data, for accurate prediction has become an urgent issue to address. Utilizing the provided EEG data, please solve the following problems:     \na) Model Construction: Explain the basic principles of building a PTSD risk assessment model based on multimodal data, including feature selection methods, algorithm selection, and how to achieve multimodal data fusion.     \nb) Model Evaluation: Provide specific analysis processes and estimation results, and evaluate the performance of the model.     \nc) Interpretation of Model Outputs: In the discussion, consider how to use the model outputs to develop personalized prediction models for early screening of rescue personnel, avoiding a 'bare-handed' approach.",
        "dataset_path": [
            "data1.xlsx",
            "A1.csv",
            "A2.csv",
            "A3.csv",
            "A4.csv",
            "A5.csv",
            "A6.csv",
            "A7.csv",
            "A8.csv",
            "A9.csv",
            "A10.csv",
            "A11.csv",
            "A12.csv",
            "A13.csv",
            "A14.csv",
            "A15.csv",
            "A16.csv",
            "A17.csv",
            "A18.csv",
            "A19.csv",
            "A20.csv",
            "A21.csv",
            "A22.csv",
            "A23.csv",
            "A24.csv",
            "A25.csv"
        ],
        "dataset_description": "The 'PCL total score' in the appendix is the response variable (dependent variable). Data Description of A1.csv to A25.csv: The name of the brain wave data corresponds to the serial number in Appendix 1. For example, the brain wave data corresponding to the sample with serial number 'A1' in data1.xlsx is A1.csv. In the brain wave data, the first column represents the data collection date; the second column represents the specific time; the third column indicates whether the electrode is connected normally, where 0 means normal connection; the fourth column represents the concentration value; the fifth column represents the relaxation value; columns 6 to 13 represent 8 types of brain wave data.",
        "variable_description": [
            {
                "Gender": "1: Male; 2: Female",
                "Educational level": "1: College and above; 2: High school; 3: Technical secondary school and below",
                "Marital status": "1: Unmarried; 2: Married; 3: Other",
                "Per capita monthly household income": "1: Below 3000; 2: 3000-4999; 3: 5000-8000; 4: Above 8000",
                "Whether an accident occurred recently": "1: No; 2: Yes",
                "Whether a family member or close friend had an accident": "1: No; 2: Yes",
                "Whether someone was witnessed being seriously injured in the accident": "1: No; 2: Yes",
                "Whether you came into contact with or saw a corpse during the accident and rescue process": "1: No; 2: Yes",
                "Have you ever felt scared, nervous or helpless because of the scenes at the disaster site (such as fire, thick smoke, blood, corpses, severe damage to buildings or vehicles, chaotic scenes, etc.)": "1: No; 2: Yes",
                "Whether there is a history of mental illness": "1: No; 2: Yes",
                "Whether drugs were used in the past month": "1: No; 2: Yes",
                "Whether smoking (including in the past)": "1: No; 2: Yes",
                "Smoking status": "1: Non-smoking, no passive smoking; 2: Non-smoking, passive smoking; 3: Smoking",
                "Whether drinking alcohol": "1: No; 2: Yes",
                "Nature of ASD": "0: Negative; 1: Positive",
                "Genetic history": "1: Negative; 2: Positive"
            }
        ],
        "addendum": "",
        "data_num": 26,
        "source": "MAS2024b",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Clinical Psychologist with Biostatistical Expertise",
                "details": "As a clinical psychologist with biostatistical expertise, you specialize in PTSD pathogenesis, psychometric assessment, and advanced statistical modeling of mental health outcomes. Your role is to evaluate the depth of analysis and rationality of reasoning in solutions. For factor analysis, you will assess whether protective and risk factors are analyzed using appropriate statistical methods (e.g., regression analysis, mediation/moderation tests) with sufficient power, and whether their mechanisms (e.g., social support as a buffer, trauma exposure severity pathways) are grounded in empirical PTSD literature (e.g., DSM-5 criteria, neurobiological models of fear conditioning). For the risk assessment model, you will examine the rationality of feature selection (e.g., whether EEG spectral features or heart rate variability align with PTSD neurophysiology), the statistical validity of multimodal fusion (e.g., justification for concatenation vs. attention-based integration), and whether model evaluation metrics (e.g., AUC, calibration curves) are interpreted in the context of clinical utility (e.g., minimizing false negatives for early screening). You will ensure reasoning links statistical findings to clinical relevance, avoiding overinterpretation of correlations as causation."
            },
            {
                "name": "Multimodal Data Fusion & Computational Psychiatry Innovator",
                "details": "As an expert in multimodal data fusion and computational psychiatry, you focus on innovative methods for integrating heterogeneous data (structured clinical variables, neurophysiological signals) to advance mental health prediction. Your role is to evaluate the innovation of solutions. For the factor analysis, you will assess if interactions between factors (e.g., genetic history × trauma exposure) are modeled using novel approaches (e.g., machine learning-based interaction detection beyond traditional ANOVAs). For the risk assessment model, you will examine innovation in multimodal data integration: whether the solution leverages cutting-edge techniques (e.g., graph neural networks for EEG feature embedding, transformer models for cross-modal attention between clinical data and brain waves) instead of conventional concatenation. You will also evaluate if the personalized prediction model introduces novel applications (e.g., real-time PTSD risk scoring via portable EEG devices for rescue personnel) or if the model interpretation (e.g., SHAP values linked to specific EEG biomarkers) advances precision psychiatry. Your expertise ensures the solution pushes beyond existing PTSD prediction models by innovatively harnessing multimodal data's predictive power."
            }
        ]
    },
    {
        "id": 19,
        "title": "Automotive Customer Satisfaction Evaluation Case Study",
        "background": "In the increasingly competitive automotive market, customer satisfaction has become a critical factor for automotive companies to establish a foothold and achieve sustainable development. In-depth analysis of the influencing factors of customer satisfaction and the construction of effective evaluation and prediction models can not only help automotive companies optimize their products and services but also significantly enhance their market competitiveness.",
        "problem_requirement": "Based on the provided automotive customer satisfaction dataset (Data.xlsx) and other publicly available automotive-related data, and in accordance with the writing standards and requirements of statistical case studies for automotive customer satisfaction evaluation, please complete the following tasks:  \n1. Follow the principles of 'data-driven, differentiated weighting, and interpretable mechanisms' to construct an automotive customer satisfaction evaluation model. Explore the impact of quality reliability, performance design, sales service, and after-sales service on satisfaction, as well as the interactions among these factors and elements. Provide practical methods to reflect perceived quality.  \n2. Based on the provided open-ended questions and diverse data sources such as user review text from automotive websites and forums, as well as vehicle sales data, integrate multi-dimensional data with customer satisfaction data to construct an automotive customer satisfaction model. Explain its basic principles, specific processes, key influencing factors, and technical methods.  \n3. Traditional automotive industry customer satisfaction surveys often use a ten-point scale, but this evaluation mechanism is somewhat cumbersome. Explore conversion schemes for transforming the ten-point scale into a binary method (satisfied/dissatisfied) and a five-point scale, as well as the analysis methods for satisfaction after conversion.  \nAttachment Description: Data.xlsx includes indicators such as satisfaction, brand image, perceived quality, perceived value, expected quality, loyalty, complaint rate, and demographic characteristics. Perceived quality includes the detailed indicators in the following table:  \nFirst-Level Indicator Second-Level Indicator (Observed Variables) Data Source\nQuality Reliability Includes 11 major factors, number of faults per 100 new vehicles 3-12 month users\nPerformance Design Includes 11 major factors, 127 elements 3-12 month users\nSales Service Quality Includes 6 major factors, 84 elements Within 3 months users.\n After-Sales Service Quality Includes 5 major factors, 58 elements 12-36 month users.",
        "dataset_path": [
            "data1.xlsx"
        ],
        "dataset_description": "",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 1,
        "source": "MAS2025",
        "source_type": "MAS",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Automotive Customer Satisfaction Quantitative Modeling Expert",
                "details": "As an expert in quantitative modeling for automotive customer satisfaction, you specialize in statistical analysis, structural equation modeling (SEM), multi-source data fusion, and psychometric scaling. Your role is to evaluate the depth of analysis by assessing whether the solution adheres to 'data-driven, differentiated weighting, and interpretable mechanisms' principles. Specifically, you will examine: 1) The rationality of factor interactions (e.g., whether Quality Reliability moderates the impact of Performance Design on satisfaction, supported by statistical tests like interaction terms in regression or SEM); 2) Practicality of perceived quality measurement (e.g., integration of objective fault data from 3-12 month users with subjective survey scores via weighted averaging or latent variable modeling); 3) Validity of multi-dimensional data integration (e.g., whether text mining of user reviews extracts meaningful sentiment/topics aligned with sales service elements, and if sales data is linked to satisfaction via causal inference methods like propensity score matching); 4) Statistical rigor of scale conversion (e.g., binary conversion using optimal cutoff points via ROC analysis, five-point scale via anchor-based mapping with test-retest reliability, and post-conversion analysis methods like ordinal logistic regression for five-point scales). You ensure reasoning is grounded in empirical evidence and methodological soundness."
            },
            {
                "name": "Automotive Customer Experience Innovation Methodologist",
                "details": "As an innovation-focused methodologist in automotive customer experience, you specialize in emerging analytical techniques, cross-domain data fusion, and user-centric measurement innovation. Your role is to evaluate the novelty of the solution by identifying creative approaches to model construction, data integration, and scale conversion. Key areas include: 1) Innovative model interpretability (e.g., using SHAP/LIME values instead of traditional regression coefficients to explain factor importance, or network analysis to visualize interactions between 127 Performance Design elements); 2) Multi-source data fusion breakthroughs (e.g., combining unstructured review text with structured survey data via transformer-based sentiment-topic modeling to uncover hidden factors like 'infotainment usability' not captured in original indicators); 3) Scale conversion innovation (e.g., adaptive binary conversion linking survey responses to actual repurchase behavior via survival analysis, or Bayesian five-point scaling accounting for user uncertainty); 4) Dynamic factor modeling (e.g., time-varying weights for After-Sales Service Quality using 12-36 month user data to capture evolving satisfaction). You highlight solutions that push beyond conventional customer satisfaction frameworks by integrating AI/ML, behavioral economics, or cross-industry best practices."
            }
        ]
    },
    {
        "id": 20,
        "title": "A Roadmap to a Better City",
        "background": "Background: \nTransportation systems can help or harm a city’s growth and the lives of its residents. A successful transportation infrastructure can attract businesses, schools, tourists, and new residents. The transportation challenges that cities face are complex and intertwined. The various stakeholders (city residents, business owners, suburban residents, commuters, passthrough travelers, tourists, etc.) have different needs and priorities within those systems. Often one element or component of a transportation system favors one stakeholder over others therefore creating an interference with other needs of the system. Highways, bus routes, and rail systems can interfere with local city bikers and pedestrians and vice versa when vehicle drivers are delayed by city walkways and traffic lights. Sometimes, the biggest obstacle to effective transportation systems is the city’s geography – water (rivers, harbors, streams, drainage runoff) or landforms (hills, ditches, valleys, slopes). Even soil composition and weather conditions can be disruptive. \nSituation:\nBaltimore, Maryland, USA is a city affected by aging infrastructure and limited transportation options that impact people’s lives and hinder its economic growth. Adding to that is the recent collapse of a major bridge (Francis Scott Key Bridge) closing a major highway across its busy harbor. Baltimore has been making plans to improve its transportation network through infrastructure improvement and enhancing its public transportation as a part of its sustainability goals. These goals are based upon identifying, prioritizing, and implementing initiatives such as collaboration between stakeholders, maintaining or updating its physical systems, using data more effectively, and seeking technological advancements that enhance the lives of its residents and visitors. \nBaltimore has a bustling port and shipping center along with being located on a major interstate highway (I-95). Several of its passthrough and commuter highways and rail lines block or interfere with streets and neighborhoods making it difficult for city residents to access job opportunities in the shipping industries and for local businesses to transport goods into and around neighborhoods. Through planning initiatives to repair roads, build bypasses, expand public transit options, and improve access to ports and airports, Baltimore hopes to facilitate commerce and make the city a better place to live, work, and visit. Recently, through national funding and support, US cities have developed infrastructure plans to remove highways that divide neighborhoods into their downtown areas and seek to replace them in ways that reconnect and revitalize those areas. While the highways allow suburban residents to commute downtown or across the city to their jobs, urban neighborhoods were separated or destroyed in the construction of these highways. Baltimore hopes to reconnect these communities and provide more sustainable communities with more green spaces, better public housing, and opportunities for community entertainment and recreation.As examples, four transportation issues are outlined in these references: \n1. The rebuilding of a collapsed bridge (Francis Scott Key Bridge) in the Harbor.[1]\n2. The inadequacy of the minimal public rail systems (MARC, light rail, heavy rail), which connect suburbs that already have several transportation options. The rail transits are notsubstantial enough to enable commuters and residents to easily use the system to get tothe workplace and the downtown free buses primarily help tourists and not the residentsof intercity communities.[2]\n3. Planning for fixing the disruption over decades on urban communities by US-40(Highway to Nowhere) through the collaborative West Baltimore United Project.[3]\n4. A travelogue of a resident of Brooklyn (community in Baltimore) and his ordeal of trying to use buses to get home after attending a football game in the city.[4]",
        "problem_requirement": "Requirements: \nAll of Baltimore’s transportation plans affect multiple stakeholders with differing perspectives.Your team’s assignment is to improve the lives of the city’s residents by recommending ways to improve Baltimore’s transportation network. A file with vehicle counts on street segments is provided. Creating transportation networks for Baltimore or one of its areas and communities will help you visualize and understand the issues. Therefore, you should build a network model(s) for some part or element of Baltimore’s transportation system.\nUsing your model, consider projects related to these transportation issues:\n1. The collapse of the Francis Scott Key Bridge had a large impact on the transportation system of Baltimore. What does your network model(s) show is the impact of the bridge collapse and/or the reconstruction of the bridge? Be sure to highlight the impacts on the various stakeholders in and around Baltimore.\n2. Many residents of the City of Baltimore walk or travel by bus. Select a project or potential project that impacts the bus or pedestrian walkway systems. What does your network model(s) show is the impact of this project? Be sure to highlight the impacts on the various stakeholders in and around Baltimore.\n3. Recommend a project for the transportation network that best improves the lives of the residents of Baltimore.\na. What are the benefits to residents of this project?\nb. How does your project impact other stakeholders?\nc. Explain the ways that your project disrupts other transportation needs and people’s lives.\nShare Your Insights\n• Safety is a significant issue facing the City of Baltimore. How can the transportation system be used to best address this issue?\n• Write a one-page memo to the Mayor of Baltimore describing two of your projects, including the benefits and drawbacks on the people and their safety in the city.",
        "dataset_path": [
            "Bus_Routes.csv",
            "Bus_Stops.csv",
            "nodes_all.csv",
            "nodes_drive.csv",
            "edges_all.csv",
            "edges_drive.csv",
            "MDOT_SHA_Annual_Average_Daily_Traffic_Baltimore.csv",
            "Edge_Names_With_Nodes.csv",
            "DataDictionary.csv"
        ],
        "dataset_description": "This question contains all 9 of the data files listed below.\n1. Bus_Routes.csv:[5] This dataset represents the locations of MTA bus routes within the City of Baltimore as of 2022.\n2. Bus_Stops.csv:[6] This dataset represents the locations of MTA Bus Stops as of 2022 within the City of Baltimore.\n3. nodes_all.csv:[7] This dataset represents the locations of tagged geographic attributes by OpenStreetMaps[8] that provide transportation data points in Baltimore. Generally, these are locations where two transportation paths (road, highway, bikeway, walkway, etc.) intersect.\n4. nodes_drive.csv:[7] This dataset represents the locations of tagged geographic attributes by OpenStreetMaps[8] for car travel. Generally, these are locations where two roads or highways intersect.\n5. edges_all.csv:[7] This dataset represents the transportation paths between two nodes from the nodes_all.csv dataset.\n6. edges_drive.csv:[7] This dataset represents the roadways between two nodes from the nodes_drive.csv dataset.\n7. MDOT_SHA_Annual_Average_Daily_Traffic_Baltimore.csv: [9] MDOT SHA Annual Average Daily Traffic (AADT) data consists of linear & point geometric features which represent the geographic locations & segments of roadway throughout the State of Maryland that include traffic volume information. Traffic volume information is produced from traffic counts used to calculate annual average daily traffic (AADT), annual average weekday traffic (AAWDT), AADT based on vehicle class (current year only) for roadways throughout the State.\n8. Edge_Names_With_Nodes.csv:[7] This dataset pairs the information from the nodes_all.csv dataset with information from the edges_all.csv dataset to provide street names with nodes.\n9. DataDictionary.csv: This data file describes the features in each of the data sets provided for this question.\nThere are many valuable data sets available at: https://baltometro.org/about-us/data-maps/regional-gis-data-center and https://opendata.baltimorecountymd.gov/",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 9,
        "source": "ICM2025d",
        "source_type": "ICM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Transportation Systems Analysis Expert",
                "details": "As a Transportation Systems Analysis Expert, you specialize in network modeling, traffic flow dynamics, and stakeholder impact assessment. Your expertise includes evaluating how transportation datasets (e.g., MDOT AADT traffic counts, OpenStreetMap nodes/edges, bus routes/stops) are integrated into network models to quantify impacts. You will assess the depth of analysis by examining if the model uses relevant metrics (congestion indices, travel time variability, accessibility scores) to measure bridge collapse/reconstruction effects, bus/pedestrian project outcomes, and recommended project benefits. You will verify the rationality of reasoning by checking if stakeholder impacts (e.g., suburban commuters’ delay, port businesses’ access issues, intercity residents’ transit reliability) are directly linked to model outputs (e.g., diverted traffic volumes, bus route frequency improvements) and if conclusions are grounded in empirical data rather than anecdotal claims."
            },
            {
                "name": "Urban Mobility Innovation Strategist",
                "details": "As an Urban Mobility Innovation Strategist, you focus on advancing transportation solutions through creative integration of technology, multi-modal connectivity, and community-centric design. Your expertise spans sustainable urban planning, smart transit systems, and equitable mobility. You will evaluate the innovation of solutions by assessing if they go beyond conventional network modeling—e.g., integrating real-time transit data with predictive analytics to optimize bus routes, using multi-layered network models (combining pedestrian walkability, bike lanes, and public transit) to identify equity gaps, or proposing novel infrastructure (e.g., ‘transit-oriented community hubs’ replacing highway dividers with green spaces and micro-mobility hubs). You will also check if innovations address Baltimore’s unique challenges (aging infrastructure, neighborhood disconnection) by leveraging underutilized datasets (e.g., bus stop proximity to affordable housing) or emerging technologies (smart traffic signals, community-driven transit apps) to enhance resident quality of life."
            }
        ]
    },
    {
        "id": 21,
        "title": "To Play or not to Play: Modeling Future Olympic Games",
        "background": "The International Olympic Committee (IOC) is planning the 2032 Summer Olympics in Brisbane, Australia. As the Olympics evolve, the IOC aims to keep the Games both relevant and impactful by adding sports, disciplines, or events (SDEs) that resonate with modern values and appeal to a global audience. Throughout Olympic history, SDEs have been introduced, removed, or even reintroduced to reflect the times. For example, in 2020, Karate, Sport Climbing, Surfing, and Skateboarding made their Olympic debut. However, Karate was no longer included in the 2024 Paris Olympics, while Breaking (also known as breakdancing) was introduced. Looking forward to the 2028 Los Angeles Olympics, Flag Football, Lacrosse, and Coastal Rowing will be added, while Baseball and Softball will return to be contested in the 2028 Games after a 20-year absence. To guide these decisions, the IOC’s Olympic Programme Commission has developed a set of criteria to help ensure that each sport aligns with Olympic values.[1] IOC Criteria for Sports Inclusion is summarized below: \n● Popularity and Accessibility: Enhances the Olympic Games’ appeal and global interestwithout excessively increasing costs or logistical demands.● Gender Equity: Ensures that both men and women athletes have equal opportunity to participate.\n● Sustainability: Promotes environmental and social responsibility.\n● Inclusivity: Represents diverse cultures and promotes global participation (at least 75countries across four continents practicing the sport).\n● Relevance and Innovation: The sport must appeal to younger audiences, reflect modern trends, and incorporate innovations including physical virtual sports where appropriate while respecting Olympic traditions.\n● Safety and Fair Play: Maintains high standards for athlete protection and anti-doping.",
        "problem_requirement": "The IOC has hired your team, HiMCM Olympic Consultants (HOC), to assist in evaluating which SDEs should be added (or potentially removed) from the 2032 Summer Games. Your task is to create a mathematical model that evaluates SDEs against these criteria to provide well-reasoned recommendations. This model will be used to make quantitatively informed decisions about which SDEs best fit the Olympics’ evolving vision. Your Tasks \n1. What factors need to be considered when addressing the IOC criteria? List and describe the various factors your team identifies. Note that factors may be quantitative or qualitative, constant or variable, and deterministic or probabilistic. Be sure to justify your choices and to include units where appropriate. \n2. Use your factors to build a model (or set of models) to help the IOC evaluate which SDEs align best with Olympic criteria. \n3. Test your model on at least three SDEs that have been added or removed from recent Olympics, namely Olympic years 2020, 2024, and 2028 and at least three SDEs that have continuously been in the Olympic programme since the 1988 games or earlier. The supplied data HiMCM_Olympic_Data.xlsx provides information about which sports and disciplines, and number of events, have appeared in each Olympics since their modern formation. Be sure to highlight the general applicability of your model by choosing a diverse collection of SDEs to evaluate. Discuss how your model affirms these SDEs’current Olympic status. \n4. Identify three SDEs that could be new additions or reintroductions for the 2032 Olympics in Brisbane. Make sure to identify which SDE should be considered first, second and third for inclusion in the Brisbane games. Are there any SDEs that you believe have potential for inclusion in an Olympic games in 2036 or beyond? \n5. Perform sensitivity analysis to address the robustness of your model. Identify what aspects of your model make an SDE score well and discuss if these represent a strength or weakness of your model, especially in light of it being used as a decision-making tool. \n6. Draft a one- to two-page letter addressed to the IOC summarizing your findings in a non-technical way. Explain your model’s rationale and its results for the evaluated SDEs. Include your recommendations for which SDEs to add or remove and why your model supports these conclusions.",
        "dataset_path": [
            "HiMCM_Olympic_Data.xlsx"
        ],
        "dataset_description": "HiMCM_Olympic_Data.xlsx – Data set of current and discontinued summer program.\nSport and Discipline: The sports (and disciplines) make up the current and discontinued Summer Olympic Games official program and are listed alphabetically according to the name used by the IOC.\nCode: Each discipline is marked with a unique 3-character identifier code by the IOC.\nSports Governing Body: A listing of the sports governing bodies associated with Olympic sports.\n1896 – 2028: The four number code in each column represents the year of the associated Olympic Games. Figures in each cell of the associated column indicate the number of events for each sport contested at the respective Games; a bullet (•) denotes that the sport was contested as a demonstration or unofficial sport",
        "variable_description": [
            {
                "Sport and Discipline": "The sports (and disciplines) make up the current and discontinued Summer Olympic Games official program and are listed alphabetically according to the name used by the IOC.",
                "Code": "Each discipline is marked with a unique 3-character identifier code by the IOC.",
                "Sports Governing Body": "A listing of the sports governing bodies associated with Olympic sports.",
                "1896 – 2028": "The four number code in each column represents the year of the associated Olympic Games. Figures in each cell of the associated column indicate the number of events for each sport contested at the respective Games; a bullet (•) denotes that the sport was contested as a demonstration or unofficial sport"
            }
        ],
        "addendum": "Glossary\nDeterministic: processes that have only one (predetermined) outcome.\nInternational Olympic Committee (IOC): is the international, non-governmental, sports\ngoverning body of the Olympic Games and the Olympic Movement. The IOC is best known as\nthe organization responsible for organizing the Summer and Winter Olympics.\nIOC’s Olympic Programme Commission: has, among other duties, the responsibility for\nanalyzing the programmes for the Summer and Winter Olympic Games.\nPhysical virtual sport: refers to a sport that integrates physical activity with virtual or digital\nelements, often through advanced technology like augmented reality (AR), virtual reality (VR),\nor esports platforms. These sports combine real-world physical exertion or skill with virtual\nsettings or competitions, bridging traditional physical sports and digital interaction. One example\nof a physical virtual sport is Zwift cycling, where athletes use stationary bikes equipped with\nsensors to track their speed and resistance. The data is then streamed into a virtual environment\nwhere cyclists \"ride\" through various digital courses, competing with other participants online.\nThis type of setup blends real physical cycling with a virtual competition, allowing for both\nathletic performance and digital engagement.\nProbabilistic: processes based on the theory of probability or that randomness plays a role in\npredicting future events.\nProgramme: of the Olympic Games is the programme of all sports competitions established by\nthe IOC for each edition of the Olympic Games.\n| ©2024 by COMAP | www.comap.org | www.mathmodels.org | info@comap.org |\nSDE: Sport, Discipline, or Event\nSport: The IOC defines an Olympic sport as a discipline that is governed by a single\ninternational sports federation (IF). A single sport may contain one or more disciplines,\neach of which is the focus of one or more events.\nDiscipline: A branch of a sport that includes one or more events.\nEvent: A competition within a discipline that results in a ranking and awards (e.g. medals).\nExample of the relationship between sport, discipline, and event in the Olympic\nprogramme from the 2024 Paris Olympics:\n● World Aquatics is the IF that governs the sport of aquatics\n● Within the sport of aquatics are multiple disciplines – artistic swimming, diving,\nmarathon swimming, swimming, and water polo.\n● Within the discipline of diving are eight medal events:\n○ Individual 3m springboard - men & women\n○ Individual 10m platform - men & women\n○ Synchronized 3m springboard - men & women\n○ Synchronized 10m platform - men & women\nSports governing body: A sports governing body is a sports organization that has a regulatory\nor sanctioning function. Sports governing bodies come in various forms and have a variety of\nregulatory functions, including disciplinary action for rule infractions and deciding on rule\nchanges in the sport that they govern. Governing bodies have different scopes. They may cover a\nrange of sports at an internationally acceptable level. An example of a sports governing body is\nan International Federation. International Sports Federations (IFs): are international non-\ngovernmental organizations recognized by the (IOC) as administering one or more sports at the\nworld level. Recognized IFs must ensure that their statutes, practice and activities conform with\nthe Olympic Charter, ultimately, they are responsible for the integrity of their sport on the\ninternational level",
        "data_num": 1,
        "source": "HiMCM2024a",
        "source_type": "HiMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Multi-Criteria Decision Analysis (MCDA) Expert",
                "details": "As a Multi-Criteria Decision Analysis Expert, you specialize in translating complex qualitative and quantitative criteria into structured, defensible evaluation models. Your expertise includes operationalizing abstract concepts (e.g., 'inclusivity', 'relevance') into measurable factors, selecting appropriate weighting methodologies (e.g., AHP, TOPSIS, or fuzzy logic), and validating model robustness. You will evaluate the depth of analysis by assessing whether all IOC criteria are comprehensively addressed, if factors (e.g., social media engagement for popularity, gender pay gap for equity) are empirically justified, and if the model’s reasoning for aggregating criteria (e.g., additive weighting vs. multiplicative interactions) aligns with decision science best practices. You will also verify that testing on historical SDEs (e.g., Karate 2020 vs. Breaking 2024) demonstrates the model’s ability to replicate or explain past IOC decisions, ensuring the model’s rationality and predictive validity."
            },
            {
                "name": "Sports Innovation & Emerging Trends Analyst",
                "details": "As a Sports Innovation & Emerging Trends Analyst, you focus on identifying cutting-edge approaches to evaluating sports relevance in a rapidly evolving global landscape. Your expertise spans integrating modern data sources (e.g., esports viewership, social media sentiment for youth appeal), leveraging probabilistic modeling for uncertain trends (e.g., future sustainability costs), and designing adaptive frameworks that account for dynamic criteria (e.g., evolving gender equity standards). You will assess innovation by examining whether the model goes beyond static weighted sums—for example, using machine learning to identify non-linear relationships between criteria, incorporating physical virtual sports metrics, or applying network analysis to map global inclusivity beyond just country counts. You will also evaluate if the model anticipates future IOC priorities (e.g., climate impact of venue logistics, digital fan engagement) and if its sensitivity analysis explores novel uncertainties (e.g., geopolitical shifts affecting inclusivity)."
            }
        ]
    },
    {
        "id": 22,
        "title": "Great Lakes Water Problem",
        "background": "The Great Lakes of the United States and Canada are the largest group of freshwater lakes in the world. The five lakes and connecting waterways constitute a massive drainage basin that contains many large urban areas in these two countries, with varied climate and localized weather conditions. The lakes’ water is used for many purposes (fishing, recreation, power generation, drinking, shipping, animal and fish habitat, construction, irrigation, etc.). Consequently, a vast variety of stakeholders have an interest in the management of the water that flows into and out of the lakes. In particular, if too little water is drained or evaporates from the lakes, then flooding may occur and homes and businesses along the shore suffer; if too much water is drained, then large ships cannot travel through the waterways to bring supplies and support the local economy. The main problem is regulating the water levels such that all stakeholders can benefit.  \nThe water level in each lake is determined by how much water enters and leaves the lake. These levels are the result of complex interactions among temperature, wind, tides, precipitation, evaporation, bathymetry (the shape of the lake bottom), river flows and runoff, reservoir policies, seasonal cycles, and long-term climate changes. There are two primary control mechanisms within the flow of water in the Great Lakes system – Compensating Works of the Soo Locks at Sault Ste. Marie (three hydropower plants, five navigation locks, and a gated dam at the head of the rapids) and the Moses-Saunders Dam at Cornwall as indicated in the Addendum.  While the two control dams, many channels and canals, and the drainage basin reservoirs may be controlled by humans, the rates of rain, evaporation, erosion, ice jams, and other water-flow phenomena are beyond human manipulation. The policies of local jurisdictions may have different effects than expected, as can seasonal and environmental changes in the water basin. These changes in turn affect the ecosystem of the area, which impacts the health of the flora and fauna found in and around the lakes and the residents that live in the water basin. Even though the Great Lakes seem to have a regular annual pattern, a variance from normal of two to three feet of water level can dramatically affect some of the stakeholders. This dynamic network flow problem is “wicked” – exceptionally challenging to solve because of interdependencies, complicated requirements, and inherent uncertainties. For the lake’s problems, we have ever-changing dynamics and the conflicting interests of stakeholders.  See Problem D Addendum for Additional Information.",
        "problem_requirement": "The International Joint Commission (IJC) requests support from your company, International network Control Modelers – ICM, to assist with management and models for the control mechanisms (the two dams – Compensating Works and Moses-Saunders Dam as indicated in the Addendum) that directly influence water levels in the Great Lakes flow network. Your ICM supervisor has given your team the lead in developing the model and a management plan to implement the model. Your supervisor indicates there are several considerations that may help to achieve this goal starting with the building of a network model for the Great Lakes and connecting river flows from Lake Superior to the Atlantic Ocean. Some other optional considerations or issues your supervisor mentioned were:  \n• Determination of the optimal water levels of the five Great Lakes at any time of the year, taking into account the various stakeholders’ desires (the costs and benefits could be different for each stakeholder).  \n• Establishment of algorithms to maintain optimal water levels in the five lakes from inflow and outflow data for the lakes. \n• Understanding of the sensitivity of your control algorithms for the outflow of the two control dams. Given the data for 2017, would your new controls result in satisfactory or better than the actual recorded water levels for the various stakeholders for that year? \n• How sensitive is your algorithm to changes in environmental conditions (e.g., precipitation, winter snowpack, ice jams)? \n• Focus your extensive analysis of ONLY the stakeholders and factors influencing Lake Ontario as there is more recent concern for the management of the water level for this lake. \nThe IJC is also interested in what historical data you use to inform your models and establish parameters, as they are curious to compare how your management and control strategies compare to previous models. Provide a one-page memo to IJC leadership communicating the key features of your model to convince them to select your model. ",
        "dataset_path": [
            "Problem_D_Great_Lakes.xlsx"
        ],
        "dataset_description": "Problem_D_Great_Lakes.xlsx: Data for the inflows, outflows, and water levels for the lakes.",
        "variable_description": [
            {}
        ],
        "addendum": "Addendum to ICM Problem D - Great Lakes Water Problem. The levels of the Great Lakes of the United States and Canada follow a rhythm: spring snow and ice melt drain into the lakes; by early summer water levels peak, then evaporation lowers levels by autumn; winter ice covers parts of lakes and rivers, with ice dams disrupting patterns. Lake levels are influenced by precipitation, evaporation, and long latency water flows from Lake Superior through to the St. Lawrence River, which can take years. Dams controlling outflows from Lakes Superior and Ontario (Compensating Works at Sault Ste. Marie and Moses-Saunders Dam at Cornwall) are regulated by the International Joint Commission (IJC) to balance stakeholder interests, using control algorithms based on inflow and outflow data to keep levels near long-term averages. Considerable inflow, outflow, and water-level data is available, with additional databases providing finer time intervals and factors such as flow rates, snowpack, forecasts, ice content, temperatures, evaporation, and weather.\n\nLake Ontario Sub-Problem: Outflow control via the Moses-Saunders Dam (Plan 2014) became controversial after record-high levels in 2017 and 2019. Stakeholders include: (1) shipping companies wanting high static water in the St. Lawrence River, (2) dock managers and residents near Montreal harbor wanting steady, low river levels, (3) environmentalists wanting seasonal highs and lows to support habitats and clean bays, (4) Lake Ontario property owners preferring mid-level, steady water, (5) recreational boaters and fishers also preferring mid-level, steady water, (6) hydro-power companies wanting high-level water storage to maximize generation. Montreal, Canada, is especially exposed due to flows of both the St. Lawrence and Ottawa Rivers, despite 50 Ottawa River dams and 13 large reservoirs storing runoff to reduce downstream flooding. Historical Ottawa River flow data is provided.\n\nPotential subproblem factors: (1) current Lake Ontario level and time of year, (2) Ottawa River flow and time of year, (3) snowpack and Ottawa River forecast, (4) St. Lawrence River water level, current, and ice downstream of Moses-Saunders, (5) Ottawa River reservoir levels, (6) levels of the other Great Lakes, (7) water temperature, evaporation, and weather. Plan 2014’s trigger-point algorithm may be inflexible if thresholds are too high or exclude data like snowpack, reservoir levels, and other Great Lakes levels.\n\nReferences: IJC Great Lakes-St. Lawrence River Basin, Vivid Maps Great Lakes Profile, International Lake Ontario-St. Lawrence River Board, US Army Corps of Engineers Great Lakes Information, NOAA Great Lakes Environmental Research Laboratory data (levels, hydrologic), Environment and Climate Change Canada water data, Great Lakes Coordinating Committee datasets, Great Lakes Commission water-use database, Ottawa River Regulation Planning Board, and USGS National Water Dashboard.",
        "data_num": 1,
        "source": "ICM2024",
        "source_type": "ICM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Hydrological Network Modeling and Stakeholder Optimization Expert",
                "details": "As a Hydrological Network Modeling and Stakeholder Optimization Expert, you possess deep expertise in hydrological system dynamics, network flow modeling, and multi-stakeholder utility analysis. Your role is to evaluate the depth and rationality of the modeling solutions by assessing: (1) the accuracy of the Great Lakes network representation (e.g., latency between Lake Superior and St. Lawrence River, interdependencies among lakes), (2) the rigor in quantifying stakeholder costs/benefits (e.g., shipping vs. shoreline property owners, hydro-power vs. environmental habitats), (3) the logical derivation of dam control algorithms (e.g., how inflow/outflow data, historical averages, and seasonal cycles inform outflow decisions), and (4) the robustness of sensitivity analyses (e.g., validation against 2017 data, responsiveness to environmental variables like ice jams or snowpack). You ensure the model’s assumptions are hydrologically sound, stakeholder trade-offs are explicitly addressed, and the reasoning linking inputs (data) to outputs (optimal levels) is transparent and evidence-based."
            },
            {
                "name": "Water Resources Innovation and Adaptive Control Specialist",
                "details": "As a Water Resources Innovation and Adaptive Control Specialist, you specialize in emerging technologies, advanced control systems, and novel methodologies for dynamic water resource management. Your role is to assess the innovation of the modeling solutions by evaluating: (1) the introduction of cutting-edge techniques (e.g., machine learning for forecasting inflows/evaporation, real-time adaptive algorithms that adjust dam operations based on short-term weather forecasts or climate change projections), (2) improvements over traditional IJC models (e.g., Plan 2014 for Lake Ontario) in integrating underutilized data streams (e.g., high-resolution ice content, snowpack forecasts, or fine-scale weather data), (3) novel approaches to multi-stakeholder optimization (e.g., game-theoretic frameworks to balance conflicting interests or participatory modeling that incorporates stakeholder input dynamically), and (4) adaptability to long-term changes (e.g., climate-driven shifts in precipitation/evaporation patterns). You identify whether the solution moves beyond static, average-based control to embrace proactive, data-driven, or AI-enhanced strategies that improve upon historical management approaches."
            }
        ]
    },
    {
        "id": 23,
        "title": "Momentum in Tennis",
        "background": "In the 2023 Wimbledon Gentlemen’s final, 20-year-old Spanish rising star Carlos Alcaraz defeated 36-year-old Novak Djokovic. The loss was Djokovic’s first at Wimbledon since 2013 and ended a remarkable run for one of the all-time great players in Grand Slams. \nThe match itself was a remarkable battle.[1] Djokovic seemed destined to win easily as he dominated the first set 6 – 1 (winning 6 of 7 games). The second set, however, was tense and finally won by Alcarez in a tie-breaker 7 – 6. The third set was the reverse of the first, Alcaraz winning handily 6 – 1. The young Spaniard seemed in total control as the fourth set started, but somehow the match again changed course with Djokovic taking complete control to win the set 6 – 3. The fifth and final set started with Djokovic carrying the edge from the fourth set, but again a change of direction occurred and Alcaraz gained control and the victory 6 – 4. The data for this match is in the provided data set, “match_id” of “2023-wimbledon-1701”. You can see all the points for the first set when Djokovic had the edge using the “set_no” column equal to 1. The incredible swings, sometimes for many points or even games, that occurred in the player who seemed to have the advantage are often attributed to “momentum.”  \nOne dictionary definition of momentum is “strength or force gained by motion or by a series of events.”[2] In sports, a team or player may feel they have the momentum, or “strength/force” during a match/game, but it is difficult to measure such a phenomenon. Further, it is not readily apparent how various events during the match act to create or change momentum if it exists. \nData is provided for every point from all Wimbledon 2023 men’s matches after the first 2 rounds. You may choose to include additional player information or other data at your discretion, but you must completely document the sources.",
        "problem_requirement": "Use the data to: \n• Develop a model that captures the flow of play as points occur and apply it to one or more of the matches. Your model should identify which player is performing better at a given time in the match, as well as how much better they are performing. Provide a visualization based on your model to depict the match flow. Note: in tennis, the player serving has a much higher probability of winning the point/game. You may wish to factor this into your model in some way. \n• A tennis coach is skeptical that “momentum” plays any role in the match. Instead, he postulates that swings in play and runs of success by one player are random. Use your model/metric to assess this claim. \nCoaches would love to know if there are indicators that can help determine when the flow of play is about to change from favoring one player to the other.  \no Using the data provided for at least one match, develop a model that predicts these swings in the match. What factors seem most related (if any)? \no Given the differential in past match “momentum” swings how do you advise a player going into a new match against a different player? \n• Test the model you developed on one or more of the other matches. How well do you predict the swings in the match? If the model performs poorly at times, can you identify any factors that might need to be included in future models? How generalizable is your model to other matches (such as Women’s matches), tournaments, court surfaces, and other sports such as table tennis. \n• Produce a report of no more than 25 pages with your findings and include a one- to two-page memo summarizing your results with advice for coaches on the role of “momentum”, and how to prepare players to respond to events that impact the flow of play during a tennis match. ",
        "dataset_path": [
            "data_dictionary.csv",
            "Wimbledon_featured_matches.csv"
        ],
        "dataset_description": "Wimbledon_featured_matches.csv – data set of Wimbledon 2023 Gentlemen’s singles matches after second round. data_dictionary.csv – description of the data set",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 2,
        "source": "MCM2024c",
        "source_type": "MCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Tennis Match Dynamics and Statistical Reasoning Expert",
                "details": "As a Tennis Match Dynamics and Statistical Reasoning Expert, you specialize in point-level tennis data analysis, time-series modeling, and the statistical evaluation of in-match performance fluctuations. Your expertise includes understanding the structural nuances of tennis (e.g., serving advantage, game/set scoring dynamics) and applying rigorous statistical methods (e.g., regression analysis, time-series decomposition, hypothesis testing for randomness). You will evaluate the depth of the solution’s model development: Does it appropriately control for the serving player’s inherent advantage (e.g., via expected point win probability baselines)? Are the statistical tests for 'random swings' (e.g., comparing observed run lengths to simulated random sequences) logically sound and sufficiently powered? You will also assess the rationality of reasoning behind variable selection (e.g., why recent 5-point windows vs. 10-point windows) and whether conclusions about momentum (or its absence) are supported by robust data analysis rather than anecdotal interpretation."
            },
            {
                "name": "Innovative Tennis Analytics and Momentum Visualization Expert",
                "details": "As an Innovative Tennis Analytics and Momentum Visualization Expert, you focus on advancing beyond traditional sports statistics to develop novel metrics, predictive frameworks, and visual tools that capture intangible dynamics like 'momentum'. Your expertise spans creative data science (e.g., novel feature engineering, hybrid statistical/machine learning models) and cutting-edge visualization techniques (e.g., dynamic, context-aware dashboards). You will evaluate the innovation of the solution: Does the model introduce a unique metric for 'performance advantage' (e.g., a normalized 'momentum score' that adjusts for serve, set context, and player strength)? Are the swing-prediction factors innovative (e.g., integrating physiological markers, crowd noise, or subtle point quality metrics like shot depth/velocity)? Does the visualization go beyond standard line graphs (e.g., interactive heatmaps showing momentum shifts by court region, or 3D timelines that embed game/set context)? You will also assess if the solution pushes boundaries by generalizing to new contexts (e.g., women’s matches with different serve dynamics) or cross-sport applicability (e.g., adapting the model to table tennis rallies) in innovative ways."
            }
        ]
    },
    {
        "id": 24,
        "title": "Understanding Used Sailboat Prices",
        "background": "Like many luxury goods, sailboats vary in value as they age and as market conditions change. The attached “2023_MCM_Problem_Y_Boats.xlsx” file includes data on approximately 3500 sailboats from 36 to 56 feet long advertised for sale in Europe, the Caribbean, and the USA in December 2020. A boating enthusiast provided these data to COMAP. Like most real-world data sets, it may have missing data or other issues that require some data cleaning prior to analysis. \nThe Excel file includes two tabs, one for monohulled sailboats and one for catamarans. In each tab, columns are labeled Make, Variant, Length (in feet), Geographic Region, Country/Region/State, Listing Price (in US dollars), and Year (of manufacture). \nFor a given make, variant, and year, there are many other sources beyond the provided Excel file that may provide detailed descriptions of the features of a particular sailboat. You may supplement the data set provided with any additional data you choose; however, you must include the data in “2023_MCM_Problem_Y_Boats.xlsx” in your modeling. Be sure to fully identify and document the source of any supplemental data used.  \nSailboats are frequently sold through brokers. In a desire to better understand the sailboat market, one sailboat broker in Hong Kong (SAR), China has commissioned your team to prepare a report on the pricing of used sailboats.",
        "problem_requirement": "The broker would like you to: \n• Develop a mathematical model that explains the listing price of each of the sailboats in the provided spreadsheet. Include any predictors you consider useful. You may draw on other sources to understand additional features of a given sailboat (such as beam, draft, displacement, rigging, sail area, hull materials, engine hours, sleeping capacity, headroom, electronics, etc.) and for economic data by year and region. Identify and describe all sources of data used. Include a discussion of the precision of your estimate for each sailboat variant’s price.  \n• Use your model to explain the effect, if any, of region on listing prices. Discuss whether any regional effect is consistent across all sailboat variants. Address the practical and statistical significance of any regional effects noted. \nDiscuss how your modeling of the given geographic regions can be useful in the Hong Kong (SAR) market. Choose an informative subset of sailboats, split between monohulls and catamarans, from the provided spreadsheet. Find comparable listing price data for that subset from the Hong Kong (SAR) market. Model what the regional effect of Hong Kong (SAR) would be, if there is one, on each of the sailboat prices for the sailboats in your subset. Is the effect the same for both catamarans and monohull sailboats? \n• Identify and discuss any other interesting and informative inferences or conclusions your team draws from the data. \n• Prepare a one- to two-page report for the Hong Kong (SAR) sailboat broker. Include a few well-chosen graphics to help the broker understand your conclusions.",
        "dataset_path": [
            "2023_MCM_Problem_Y_Boats.xlsx"
        ],
        "dataset_description": "The data file contains information on Monohulled Sailboats and Catamarans.",
        "variable_description": [
            {
                "Make": "The name of the manufacturer of the boat.",
                "Variant": "The name identifying the particular model of the boat.",
                "Length (ft)": "The length of the boat in feet.",
                "Geographic Region": "The geographic region of the boat’s location (Caribbean, Europe, USA).",
                "Country/Region/State": "The specific country/region/state of the boat’s location.",
                "Listing Price (USD)": "The advertised price to purchase the boat in U.S. Dollars.",
                "Year": "The year the boat was manufactured."
            }
        ],
        "addendum": "",
        "data_num": 1,
        "source": "MCM2023y",
        "source_type": "MCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Maritime Econometric Modeling & Pricing Validation Expert",
                "details": "As a Maritime Econometric Modeling & Pricing Validation Expert, you specialize in statistical modeling of asset pricing, data quality assessment, and inference of economic relationships in marine markets. You evaluate the depth of analysis by assessing model specification (e.g., linear vs. non-linear functional forms, inclusion of interaction terms for boat features like length-year or variant-region), data cleaning rigor (handling missing 'Variant' or 'Year' data, treatment of outliers in 'Listing Price'), and variable selection justification (e.g., why specific boat features like 'beam' or 'displacement' were included, or exclusion of irrelevant economic indicators). You also verify the rationality of reasoning by checking statistical validity of regional effect conclusions—ensuring confounding variables (e.g., boat age, local demand/supply, or condition proxies) are controlled for—and the precision of price estimates (e.g., use of confidence intervals for coefficients, prediction error metrics like RMSE or MAE, and discussion of variant-specific uncertainty). Your expertise ensures the model’s conclusions about price drivers and regional effects are statistically sound and practically meaningful."
            },
            {
                "name": "Sailboat Market Innovation & Regional Analytics Specialist",
                "details": "As a Sailboat Market Innovation & Regional Analytics Specialist, you focus on novel approaches to data integration, modeling techniques, and regional price dynamics in marine markets. You evaluate innovation by assessing the use of supplementary data sources beyond basic boat specs (e.g., Hong Kong-specific factors like marina fees, import tariffs, or brokerage transaction volumes; or economic data like regional disposable income or boating participation rates). You also examine creative modeling strategies (e.g., hierarchical Bayesian models to capture variant-specific regional sensitivities, spatial regression to account for geographic proximity effects, or machine learning models with SHAP values to isolate regional feature importance). Additionally, you assess innovation in regional effect analysis—such as testing for variant-specific regional consistency (e.g., whether catamarans show stronger regional price sensitivity than monohulls) or developing transfer learning frameworks to adapt the global model to Hong Kong’s market with limited local data. Your expertise highlights solutions that go beyond traditional linear regression, integrate unique data insights, or uncover novel patterns in regional sailboat pricing."
            }
        ]
    },
    {
        "id": 25,
        "title": "CO2 and Global Warming",
        "background": "Prior to the Industrial Revolution, carbon dioxide (CO2) in the atmosphere was consistently around 280 parts per million (ppm). The concentration of CO2 in the atmosphere reached 377.7 ppm in March of 2004, resulting in the largest 10-year average increase up to that time.[1] According to scientists from National Oceanographic and Atmospheric Administration (NOAA) and Scripps Institution of Oceanography (SIO) the monthly mean CO2 concentration level peaked at 421 ppm in May 2022.[2] An Organisation for Economic Co-Operations and Development (OECD) report predicts a CO2 level of 685 ppm by 2050.[3]  \nThe editors of Scientific Today magazine have asked your team to address these claims of the current reported and future predictions of CO2 concentration levels. They provided two data sets (CO2 Data Set 1 & Temps Data Set 2) to assist in your research. \n",
        "problem_requirement": "Requirements \n1. Do you agree with CO2 level claims? Use CO2 Data Set 1 to analyze CO2 changes.  \na. Do you agree that the March 2004 increase of CO2 resulted in a larger increase than observed over any previous 10-year period? Why or why not?  \nb. Fit various (more than one) mathematical models to the data to describe past, and predict future, concentration levels of CO2 in the atmosphere.  \nc. Use each of your models to predict the CO2 concentrations in the atmosphere in the year 2100. Do any of your models agree with claims and predictions that the CO2 concentration level will reach 685 ppm by 2050? If not by 2050, when do your models predict the concentration of CO2 reaching 685 ppm?  \nd. Which model do you consider most accurate? Why? \n2. What’s the relationship between temperature and CO2? Many scientists think that there is a relationship between warming global temperatures and the concentration of CO2 in the atmosphere. Use your work in part 1 and Temps Data Set 2 to assist in your comparison of land-ocean temperatures and CO2 concentration levels. \na. Build a model to predict future land-ocean temperatures changes. When does your model predict the average land-ocean temperature will change by 1.25°C, 1.50°C, and 2°C compared to the base period of 1951-1980? \nb. Build a model to analyze the relationship (if any) between CO2 concentrations and land ocean temperatures since 1959. Explain the relationship or justify that there is no relationship. \nc. Extend your model from part 2.b. into the future. How far into the future is your model reliable? What concerns, if any, do you have with your model’s ability to predict future CO2 concentration levels and/or land-ocean temperatures?  \nPrepare a non-technical article (1 page maximum) for Scientific Today to explain in your team’s findings and possible recommendations for the future. ",
        "dataset_path": [
            "2022_HiMCM_Data.xlsx"
        ],
        "dataset_description": "2022_HiMCM_Data.xlsx - Sheet 1: CO2 Data Set 1, Sheet 2: Temps Data Set 2",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 1,
        "source": "HiMCM2022b",
        "source_type": "HiMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Climate Data Analysis and Modeling Validation Expert",
                "details": "As a Climate Data Analysis and Modeling Validation Expert, you specialize in statistical analysis of environmental time-series data, model selection, and validation of predictive climate models. You will evaluate the depth of the solution’s analysis by assessing whether all components of the problem (e.g., 1a-d and 2a-c) are addressed comprehensively. For CO2 analysis (Part 1), you will check if the 10-year increase comparison (1a) uses rigorous statistical methods (e.g., rolling averages, p-values for trend significance) and if multiple models (linear, exponential, logistic, etc.) are tested with justification for their selection. You will validate the rationality of model accuracy claims (1d) by examining use of metrics like R², root mean square error (RMSE), or residual analysis. For temperature-CO2 relationships (Part 2), you will assess if the modeling (2a-b) accounts for temporal alignment, potential lag effects, and statistical significance of correlations. Your expertise ensures the solution’s reasoning is data-driven, methodologically sound, and conclusions are supported by robust analysis."
            },
            {
                "name": "Climate Modeling Innovation and Interdisciplinary Integration Specialist",
                "details": "As a Climate Modeling Innovation and Interdisciplinary Integration Specialist, you focus on identifying novel approaches in climate data modeling, integration of cross-disciplinary insights, and creative problem-solving. You will evaluate the solution’s innovation by examining whether it goes beyond standard linear/exponential models—for example, incorporating machine learning (e.g., ARIMA, LSTM for time-series forecasting), considering non-linear CO2 dynamics (e.g., saturation effects in carbon sinks), or integrating socioeconomic factors (e.g., emission policies) into projections. For Part 2, you will assess if the CO2-temperature relationship model (2b) innovatively accounts for complex interactions (e.g., feedback loops, ocean heat uptake) or uses advanced visualization to communicate relationships. You will also check if the non-technical article (1-page summary) presents findings in an innovative, accessible format (e.g., interactive thresholds, uncertainty quantification) and if recommendations propose forward-thinking strategies (e.g., adaptive policy frameworks tied to model projections). Your expertise highlights solutions that push beyond conventional modeling to address real-world complexity."
            }
        ]
    },
    {
        "id": 26,
        "title": "The Influence of Music",
        "background": "Music has been part of human societies since the beginning of time as an essential component of cultural heritage. As part of an effort to understand the role music has played in the collective human experience, we have been asked to develop a method to quantify musical evolution. There are many factors that can influence artists when they create a new piece of music, including their innate ingenuity, current social or political events, access to new instruments or tools, or other personal experiences. Our goal is to understand and measure the influence of previously produced music on new music and musical artists. \nSome artists can list a dozen or more other artists who they say influenced their own musical work. It has also been suggested that influence can be measured by the degree of similarity between song characteristics, such as structure, rhythm, or lyrics. There are sometimes revolutionary shifts in music, offering new sounds or tempos, such as when a new genre emerges, or there is a reinvention of an existing genre (e.g. classical, pop/rock, jazz, etc.). This can be due to a sequence of small changes, a cooperative effort of artists, a series of influential artists, or a shift within society. \nMany songs have similar sounds, and many artists have contributed to major shifts in a musical genre. Sometimes these shifts are due to one artist influencing another. Sometimes it is a change that emerges in response to external events (such as major world events or technological advances). By considering networks of songs and their musical characteristics, we can begin to capture the influence that musical artists have on each other. And, perhaps, we can also gain a better understanding of how music evolves through societies over time. ",
        "problem_requirement": "Your team has been identified by the Integrative Collective Music (ICM) Society to develop a model that measures musical influence. This problem asks you to examine evolutionary and revolutionary trends of artists and genres. To do this, your team has been given several data sets by the ICM:  \n1)  “influence_data” 1 represents musical influencers and followers, as reported by the artists themselves, as well as the opinions of industry experts. These data contains influencers and followers for 5,854 artists in the last 90 years.  \n2) “full_music_data”2 provides 16 variable entries, including musical features such as danceability, tempo, loudness, and key, along with artist_name and artist_id for each of 98,340 songs. These data are used to create two summary data sets, including:  \na. mean values by artist “data_by_artist”,  \nb. means across years “data_by_year”.  \nNote: DATA provided in these files are a subset of larger data sets. These files CONTAIN THE ONLY DATA YOU SHOULD USE FOR THIS PROBLEM. \nTo carry out this challenging project, the ICM Society asks your teams to explore the evolution of music through the influence across musical artists over time, by doing the following: \n-  Use the influence_data data set or portions of it to create a (multiple) directed network(s) of musical influence, where influencers are connected to followers. Develop parameters that capture ‘music influence’ in this network. Explore a subset of musical influence by creating a subnetwork of your directed influencer network. Describe this subnetwork. What do your ‘music influence’ measures reveal in this subnetwork?  \n- Use full_music_data and/or the two summary data sets (with artists and years) of music characteristics, to develop measures of music similarity. Using your measure, are artists within genre more similar than artists between genres?  \n- Compare similarities and influences between and within genres. What distinguishes a genre and how do genres change over time? Are some genres related to others?  \n- Indicate whether the similarity data, as reported in the data_influence data set, suggest that the identified influencers in fact influence the respective artists. Do the ‘influencers’ actually affect the music created by the followers? Are some music characteristics more ‘contagious’ than others, or do they all have similar roles in influencing a particular artist’s music?  \n- Identify if there are characteristics that might signify revolutions (major leaps) in musical evolution from these data? What artists represent revolutionaries (influencers of major change) in your network?  \n- Analyze the influence processes of musical evolution that occurred over time in one genre. Can your team identify indicators that reveal the dynamic influencers, and explain how the genre(s) or artist(s) changed over time?  \n- How does your work express information about cultural influence of music in time or circumstances? Alternatively, how can the effects of social, political or technological changes (such as the internet) be identified within the network?  \nWrite a one-page document to the ICM Society about the value of using your approach to understanding the influence of music through networks. Considering the two problem data sets were limited to only some genres, and subsequently to those artists common to both data sets, how would your work or solutions change with more or richer data? Recommend further study of music and its effect on culture. \nThe ICM Society, an interdisciplinary and diverse group from the fields of music, history, social science, technology, and mathematics, looks forward to your final report.",
        "dataset_path": [
            "influence_data.csv",
            "full_music_data.csv",
            "data_by_artist.csv",
            "data_by_year.csv"
        ],
        "dataset_description": "We provide the following four data files for this problem. THE DATA FILES PROVIDED CONTAIN THE ONLY DATA YOU SHOULD USE FOR THIS PROBLEM.\n1. influence_data.csv\n2. full_music_data.csv\n3. data_by_artist.csv\n4. data_by_year.csv\nData Descriptions\n1. influence_data.csv\n(Data is encoded in utf-8 to allow for handling of special characters):\n- influencer_id: A unique identification number given to the person listed as influencer. (string of digits)\n- influencer_name: The name of the influencing artist as given by the follower or industry experts. (string)\n- influencer_main_genre: The genre that best describes the bulk of the music produced by the influencing artist. (if available) (string)\n- influencer_active_start: The decade that the influencing artist began their music career. (integer)\n- follower_id: A unique identification number given to the artist listed as follower. (string of digits)\n- follower_name: The name of the artist following an influencing artist. (string)\n- follower_main_genre: The genre that best describes the bulk of the music produced by the following artist. (if available) (string)\n- follower_active_start: The decade that the following artist began their music career. (integer)\n2. full_music_data.csv 3. data_by_artist.csv 4. data_by_year.csv\nSpotify audio features from the “full_music_data”, “data_by_artist”, “data_by_year”:\n- artist_name: The artist who performed the track. (array)\n- artist_id: The same unique identification number given in the influence_data.csv file. (string of digits)\nCharacteristics of the music:\n- danceability: A measure of how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. (float)\n- energy: A measure representing a perception of intensity and activity. A value of 0.0 is least intense/energetic and 1.0 is most intense/energetic. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. (float)\n- valence: A measure describing the musical positiveness conveyed by a track. A value of 0.0 is most negative and 1.0 is most positive. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). (float)\n- tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. (float)\n- loudness: The overall loudness of a track in decibels (dB). Values typical range between -60 and 0 db. Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). (float)\n- mode: An indication of modality (major or minor), the type of scale from which its melodic content is derived, of a track. Major is represented by 1 and minor is 0.\n- key: The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value for key is -1. (integer)\nType of vocals:\n- acousticness: A confidence measure of whether the track is acoustic (without technology enhancements or electrical amplification). A value of 1.0 represents high confidence the track is acoustic. (float)\n- instrumentalness: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. (float)\n- liveness: Detects the presence of an audience in a track. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. (float)\n- speechiness: Detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. (float)\n- explicit: Detects explicit lyrics in a track (true (1) = yes it does; false (0) = no it does not OR unknown). (Boolean)\nDescription:\n- duration_ms: The duration of the track in milliseconds. (integer)\n- popularity: The popularity of the track. The value will be between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played more frequently now will have a higher popularity than songs that were played more frequently in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity are derived mathematically from track popularity. (integer)\n- year: The year of release of a track. (integer from 1921 to 2020)\n- release_date: The calendar date of release of a track mostly in yyyy-mm-dd format, however precision of date may vary and some just given as yyyy.\n- song_title (censored): The name of the track. (string) Software was run to remove any potential explicit words in the song title.\n- count: The number of songs a particular artist is represented in the full_music_data.csv file. (integer)",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 4,
        "source": "ICM2021d",
        "source_type": "ICM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Music Network & Feature Analytics Expert",
                "details": "As a Music Network & Feature Analytics Expert, you possess expertise in directed network modeling, multivariate statistical analysis, and music informatics. You evaluate the depth and rationality of solutions by assessing the rigor of influence network parameters (e.g., centrality metrics, temporal lag considerations between influencers and followers) and the appropriateness of music similarity measures (e.g., cosine similarity, Mahalanobis distance) for features like danceability, tempo, and valence. Your analysis ensures that the reasoning connecting self-reported influence data (influence_data) to objective music feature data (full_music_data, data_by_artist) is statistically valid, that genre similarity conclusions are supported by robust comparative statistics, and that the integration of network structure and musical characteristics is logically coherent. You verify that influence measures account for confounding variables (e.g., overlapping active periods, genre baseline trends) and that subnetwork descriptions are representative of broader musical evolution patterns."
            },
            {
                "name": "Innovative Musical Evolution Modeling Specialist",
                "details": "As an Innovative Musical Evolution Modeling Specialist, you specialize in creative data science methodologies, interdisciplinary integration of network analysis and musicology, and novel approach development for measuring cultural influence. You evaluate the innovation of solutions by assessing whether they go beyond traditional network metrics (e.g., degree centrality) to propose new influence parameters (e.g., weighted edges combining self-reported influence with feature similarity scores). You look for creative use of temporal dynamics (e.g., dynamic network analysis to track influence shifts over decades) or advanced techniques (e.g., machine learning clustering of artists by feature trajectories to identify unreported influences). Your expertise ensures that solutions address limitations of the data (e.g., limited genres, missing influence links) with innovative workarounds, such as using transfer learning from similar genres or graph neural networks to predict latent influences. You also assess if the approach for detecting musical revolutions (e.g., change point analysis on feature variance or network modularity shifts) offers original insights into how genres transform or interact."
            }
        ]
    },
    {
        "id": 27,
        "title": "Funding Biodiversity Conservation",
        "background": "Background: Thousands of species of plants and animals face threats that could result in their extinction, while biodiversity conservation actions that could save them are often available. Conservation managers face difficult decisions when there is limited funding for biodiversity conservation. Managers need to decide which projects they should fund to best achieve their objectives, not the least of which is to save the most species. This is particularly important if the benefits of certain conservation actions vary across projects, the costs of these actions for specific projects differ, and the available funding for these actions is considerably less than is needed to support all proposed projects. \nOne issue that makes this a more difficult problem is the varying timeline and life cycle of each project. Conservation projects can take years or decades, and the schedule of when costs are incurred during the project can vary significantly depending on the project’s scope, location, targeted species, and responsible agency. This means that budgets for conservation projects must provide adequate funds for the entire lifetime of the project. Additionally, managers need to closely monitor their budgets to efficiently allocate funds as demand for resources vary across projects and time. For example, they need to ensure they have adequate resources available for times when some projects require more funding, and take advantage of times when some projects require less.  \nObjective: Determine how to efficiently invest in biodiversity conservation activities for endangered and threatened species that take place over long time frames, and whose expected costs change over that time. \nHiMCM Case: Prioritize Action and Funding for Plant Conservation in Florida (USA) Florida is one of the hotspots for plant biodiversity conservation, with 20% of its species imperiled. Only 2% of these plant species (~64) receive protection under the US Endangered Species Act (ESA), but funding is currently inadequate for protecting even this limited number of species. The Florida Rare Plant Conservation Endowment (FRPCE) (see Attachment A) is a trust fund spearheaded by conservation managers to provide funds over time to support research, protection, and conservation of rare and imperiled plant species found in Florida. This trust fund aims to generate an adequate endowment to pay for both the up-front costs of conservation and the long-term costs that are currently difficult to meet with traditional fundraising campaigns.",
        "problem_requirement": "Requirements:  \n1. Given the need to recover imperiled plant species in Florida and manage them into the future, develop a model to advise the FRPCE Board of the minimum fundraising required for “long term and reliable” funding. For the purposes of this HiMCM problem, we will limit our analysis to the 48 imperiled species included in the attached data set (Attachment B). Note that each row in the database indicates a recovery project for a species. \na. Identify and discuss relevant objectives that the FRPCE Board should consider in their conservation efforts and budgeting decisions. Using these objectives, what measures would you use to evaluate a proposed fundraising plan as being the “best?” \nb. List and address some general characteristics of imperiled plant species. Indicate the factors involved in these species’ protection that you will use in your decision model. \nc. Develop a model or algorithm (or set of models or algorithms) for the FRPCE Board to use to determine a fundraising schedule (money required with timeline) that will minimize the funds required to be raised, yet still obtain the necessary resources to implement the recovery projects for our 48 species and manage them into the future. \n2. Apply your model to recommend to the FRPCE Board a priority order of funding for the recovery projects that will manage balancing the funds available with the spending required on these projects over time. Discuss your recommendation.  \n3. Write a one-page non-technical memo to the FRPCE Board explaining your results, and make recommendations based on your modeling and analysis.",
        "dataset_path": [
            "HiMCM2020ProblemB_ThreatenedPlantsData.xlsx"
        ],
        "dataset_description": "Attachment B: HiMCM2020ProblemB_ThreatenedPlantsData.xlsx",
        "variable_description": [
            {
                "unique_id": "the unique plant identifier for that species.",
                "Benefit": "a measure that indicates the expected relative conservation value of funding one species over another. It takes into account information about how threatened a species is and how easy it would be to perform the conservation actions to recover it.",
                "Taxonomic Uniqueness": "a measure of the uniqueness of the species; larger number indicates greater uniqueness.",
                "Feasibility of Success": "the probability that the species will be protected from extinction if all of the actions receive funding.",
                "Year “n” cost": "the estimated recovery cost in US dollars for each row’s species in the “nth” year of that project; n ∈ {1, 2, 3, … 25}."
            }
        ],
        "addendum": " Attachment A - Biodiversity: the amount of diversity or variety between different plants, animals, and other species in a given habitat at a particular time.\nBiodiversity Conservation: the practice of protecting and preserving the great variety of species, habitats, ecosystems, and genetic diversity on the planet.\nImperiled Species: those species whose populations have decreased so dramatically that they are at risk of extinction.\nTraditional Fundraising: fundraising efforts or campaigns that occur annually or on a regular schedule that involve requests for donations to attain a specific total monetary goal.\nCOMAP thanks researchers from Arizona and Florida for their assistance in developing this project. Upon completion of HiMCM, COMAP will recognize these researchers and their institutions on the HiMCM webpage and in our spring Consortium publication.\nAttachment A\nA Conservation Endowment for imperiled plants in Florida\nFlorida has a rich and unique natural diversity, with about 18% of native U.S. plant taxa. But with ongoing threats, the number of imperiled species has increased and the State has been considered a conservation priority region. Less than 5% of imperiled plant species in Florida receive protection under the U.S. Endangered Species Act, and only a small portion of actions needed to recover Florida plants have been financed. Because existing plant conservation funding is inadequate to support research, protection, and management of imperiled plants, a group of conservation specialists representing seven institutions in Florida began conversations in 2015 to initiate the Florida Rare Plant Conservation Endowment (FRPCE). The FRPCE is being established as a mechanism to provide long-term and reliable funding to support conservation-related projects for Florida imperiled plant species and their ecosystems.\nExcerpt from: Negron-Ortiz, V. (2019, July 27-31). A Conservation Endowment for imperiled plants in Florida [Conference session]. Botany 2019 Conference: Sky Islands and Desert Seas, Tucson, Arizona. The Botanical Society of America. Printed with permission from the author.",
        "data_num": 1,
        "source": "HiMCM2020b",
        "source_type": "HiMCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Conservation Finance & Prioritization Specialist",
                "details": "As a Conservation Finance & Prioritization Specialist, you possess expertise in both conservation biology and long-term financial planning for environmental endowments. Your role is to evaluate the depth and rationality of the solution by assessing how well biological and financial factors are integrated. You will analyze whether the model appropriately incorporates species-specific characteristics (e.g., 'Benefit,' 'Taxonomic Uniqueness,' 'Feasibility of Success') into funding decisions, ensuring these biological metrics drive prioritization logically. Additionally, you will verify the financial reasoning: Does the fundraising schedule account for the 25-year variable cost timeline of each project? Is the 'minimum fundraising' calculation grounded in realistic assumptions about cash flow (e.g., handling years with high project costs, ensuring 'long term and reliable' funding by avoiding underfunding in later project phases)? You will also assess if the model defines 'reliable' funding clearly (e.g., accounting for inflation, contingency buffers, or endowment sustainability beyond the initial 25 years) and if tradeoffs between minimizing fundraising and maximizing conservation outcomes are explicitly justified."
            },
            {
                "name": "Innovative Conservation Optimization Modeler",
                "details": "As an Innovative Conservation Optimization Modeler, you specialize in applying advanced analytical and computational methods to solve complex conservation resource allocation problems. Your focus is to evaluate the innovation of the solution by assessing whether it goes beyond traditional cost-benefit or static ranking approaches. You will examine if the model integrates novel techniques such as multi-objective optimization (balancing 'minimize fundraising' with 'maximize species recovery'), stochastic programming (accounting for uncertainty in 'Feasibility of Success' or future costs), or dynamic prioritization algorithms (adjusting funding based on project phase or changing species status). Additionally, you will look for creativity in handling long-term funding challenges—for example, using machine learning to predict cost trajectories, or game-theoretic approaches to balance competing species needs. You will also assess if the solution introduces new metrics or decision-support tools (e.g., interactive dashboards for the FRPCE Board) that enhance practicality and adaptability, ensuring the model is not only mathematically sound but also pushes the boundaries of conventional conservation planning."
            }
        ]
    },
    {
        "id": 28,
        "title": "Teaming Strategies",
        "background": "As societies become more interconnected, the set of challenges they face have become increasingly complex. We rely on interdisciplinary teams of people with diverse expertise and varied perspectives to address many of the most challenging problems. Our conceptual understanding of team success has advanced significantly over the past 50+ years allowing for better scientific, creative, or physical teams to address these complex issues. Researchers have reported on best strategies for assembling teams, optimal interactions among teammates, and ideal leadership styles. Strong teams across all sectors and domains are able to perform complex tasks unattainable through either individual efforts or a sequence of additive contributions of teammates.  \nOne of the most informative settings to explore team processes is in competitive team sports. Team sports must conform to strict rules that may include, but are not limited to, the number of players, their roles, allowable contact between players, their location and movement, points earned, and consequences of violations. Team success is much more than the sum of the abilities of individual players. Rather, it is based on many other factors that involve how well the teammates play together. Such factors may include whether the team has a diversity of skills (one person may be fast, while another is precise), how well the team balances between individual versus collective performance (star players may help leverage the skills of all their teammates), and the team’s ability to effectively coordinate over time (as one player steals the ball from an opponent, another player is poised for offense).",
        "problem_requirement": "In light of your modeling skills, the coach of the Huskies, your home soccer (known in Europe and other places as football) team, has asked your company, Intrepid Champion Modeling (ICM), to help understand the team’s dynamics. In particular, the coach has asked you to explore how the complex interactions among the players on the field impacts their success. The goal is not only to examine the interactions that lead directly to a score, but to explore team dynamics throughout the game and over the entire season, to help identify specific strategies that can improve teamwork next season. The coach has asked ICM to quantify and formalize the structural and dynamical features that have been successful (and unsuccessful) for the team. The Huskies have provided data[1] detailing information from last season, including all 38 games they played against their 19 opponents (they played each opposing team twice). Overall, the data covers 23,429 passes between 366 players (30 Huskies players, and 336 players from opposing teams), and 59,271 game events. \nTo respond to the Huskie coach’s requests, your team from ICM should use the provided data to address the following: \n- Create a network for the ball passing between players, where each player is a node and each pass constitutes a link between players. Use your passing network to identify network patterns, such as dyadic and triadic configurations and team formations. Also consider other structural indicators and network properties across the games. You should explore multiple scales such as, but not limited to, micro (pairwise) to macro (all players) when looking at interactions, and time such as short (minute-to-minute) to long (entire game or entire season).  \n- Identify performance indicators that reflect successful teamwork (in addition to points or wins) such as diversity in the types of plays, coordination among players or distribution of contributions. You also may consider other team level processes, such as adaptability, flexibility, tempo, or flow. It may be important to clarify whether strategies are universally effective or dependent on opponents’ counter-strategies. Use the performance indicators and team level processes that you have identified to create a model that captures structural, configurational, and dynamical aspects of teamwork. Use the insights gained from your teamwork model to inform the coach about what kinds of structural strategies have been effective for the Huskies. Advise the coach on what changes the network analysis indicates that they should make next season to improve team success.  \n- Your analysis of the Huskies has allowed you to consider group dynamics in a controlled setting of a team sport. Understanding the complex set of factors that make some groups perform better than others is critical for how societies develop and innovate. As our societies increasingly solve problems involving teams, can you generalize your findings to say something about how to design more effective teams? What other aspects of teamwork would need to be captured to develop generalized models of team performance?",
        "dataset_path": [
            "fullevents.csv",
            "matches.csv",
            "passingevents.csv"
        ],
        "dataset_description": "ICM Network Problem\\n\\nFor this problem we have provided three data tables:\\n1) matches.csv\\n2) passingevents.csv\\n3) fullevents.csv\\n\\n===========================================\\nData Descriptions\\n===========================================\\n\\n1) matches.csv\\n~~~~~~~~~~~~~~~~~~~~~\\n\\nMatchID\\nA unique identifier for each match played during the season, and reflects the order of the match in the season.\\n\\nOpponentID\\nA unique identifier for the opposing team played in the match.  Note that the Huskies play each opposing team twice during the season.\\n\\nOutcome\\nResult of the match, either \"win\", \"loss\", or \"tie\".\\n\\nOwnScore\\nNumber of goals scored by the Huskies.\\n\\nOpponentScore\\nNumber of goals scored by the Opposing Team.\\n\\nSide\\nWhether the Huskies were the \"home\" team or \"away\" team.\\n\\nCoachID\\nA unique identifier for the Huskies coach for this match.\\n\\n\\n2) passingevents.csv\\n~~~~~~~~~~~~~~~~~~~~~\\n\\nMatchID\\nA unique identifier for each match played during the season (see matches.csv).\\n\\nTeamID\\nA unique identifier for the team involved in the pass (either \"Huskies\" or OpponentID from matches.csv).\\n\\nOriginPlayerID\\nA unique identifier for the Player at the origin of the pass.  The PlayerID has the form \"TeamID_PlayerPosition##\" where \"TeamID\" denotes the team on which the player plays and PlayerPosition reflects the player\"s position.  Possible positions are: \"F\":forward, \"D\":defense, \"M\":midfield, or \"G\":goalkeeper\\n\\nDestinationPlayerID\\nA unique identifier for the Player at the destination of the pass. (see OriginPlayerID)\\n\\nMatchPeriod\\nThe half in which the event took place.  \"1H\": first half, \"2H\": second half\\n\\nEventTime\\nThe time in seconds during the MatchPeriod (1st or 2nd half) at which the event took place.\\n\\nEventSubType\\nThe type of pass made. Can be one of: \"Head pass\", \"Simple pass\", \"Launch\", \"High pass\", \"Hand pass\", \"Smart pass\", \"Cross\".\\n\\nEventOrigin_x\\nThe x-coordinate on the field at which the pass originated. The x-coordinate is in the range [0, 100] and is oriented from the perspective of the attacking team, where 0 indicates the team\"s own goal, and 100 indicates the opposing team\"s goal.\\n\\nEventOrigin_y\\nThe y-coordinate on the field at which the pass originated. The y-coordinate is in the range [0, 100] and is oriented from the perspective of the attacking team, where 0 indicates the team\"s left-hand side, and 100 indicates the team\"s right-hand side.\\n\\nEventDestination_x\\nThe x-coordinate on the field at the pass destination.  (see EventOrigin_x)\\n\\nEventDestination_y\\nThe y-coordinate on the field at the pass destination.  (see EventOrigin_y)\\n\\n\\n2) fullevents.csv\\n~~~~~~~~~~~~~~~~~~~~~\\n\\nMatchID\\nA unique identifier for each match played during the season (see matches.csv).\\n\\nTeamID\\nA unique identifier for the team involved in the pass (either \"Huskies\" or OpponentID from matches.csv).\\n\\nOriginPlayerID\\nA unique identifier for the Player initiating the event.  The PlayerID has the form \"TeamID_PlayerPosition##\" where \"TeamID\" denotes the team on which the player plays and PlayerPosition reflects the player\"s position.  Possible positions are: \"F\":forward, \"D\":defense, \"M\":midfield, or \"G\":goalkeeper\\n\\nDestinationPlayerID\\nA unique identifier for the Player at the destination of the event. (see OriginPlayerID)\\nNOTE: Only valid for \"Pass\" or \"Substitution\" event types, otherwise NaN.\\n\\nMatchPeriod\\nThe half in which the event took place.  \"1H\": first half, \"2H\": second half\\n\\nEventTime\\nThe time in seconds during the MatchPeriod (1st or 2nd half) at which the event took place.\\n\\nEventType\\nThe type of the event. Can be one of: \"Free Kick\", \"Duel\", \"Pass\", \"Others on the ball\", \"Foul\", \"Goalkeeper leaving line\", \"Offside\", \"Save attempt\", \"Shot\", \"Substitution\", \"Interruption\"\\n\\nEventSubType\\nThe subtype of the event. Can be one of: \"Goal kick\", \"Air duel\", \"Throw in\", \"Head pass\", \"Ground loose ball duel\", \"Simple pass\", \"Launch\", \"High pass\", \"Touch\", \"Ground defending duel\", \"Hand pass\", \"Ground attacking duel\", \"Foul\", \"Free kick cross\", \"Goalkeeper leaving line\", \"\", \"Free Kick\", \"Smart pass\", \"Cross\", \"Save attempt\", \"Corner\", \"Clearance\", \"Shot\", \"Acceleration\", \"Reflexes\", \"Substitution\", \"Late card foul\", \"Simulation\", \"Free kick shot\", \"Protest\", \"Hand foul\", \"Penalty\", \"Violent Foul\", \"Whistle\", \"Out of game foul\", \"Ball out of the field\", \"Time lost foul\"\\n\\nEventOrigin_x\\nThe x-coordinate on the field at which the event originated. The x-coordinate is in the range [0, 100] and is oriented from the perspective of the attacking team, where 0 indicates the team\"s own goal, and 100 indicates the opposing team\"s goal.\\n\\nEventOrigin_y\\nThe y-coordinate on the field at which the event originated. The y-coordinate is in the range [0, 100] and is oriented from the perspective of the attacking team, where 0 indicates the team\"s left-hand side, and 100 indicates the team\"s right-hand side.\\n\\nEventDestination_x\\nThe x-coordinate on the field at the event destination.  (see EventOrigin_x)\\n\\nEventDestination_y\\nThe y-coordinate on the field at the event destination.  (see EventOrigin_y)\\n\\nNOTE: For \"Substitution\" events, the Outgoing player is the OriginPlayerID, while the incoming player is the DestinationPlayerID",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 3,
        "source": "ICM2020d",
        "source_type": "ICM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Network Science and Sports Dynamics Analyst",
                "details": "As a Network Science and Sports Dynamics Analyst, you specialize in applying graph theory, temporal network analysis, and sports performance metrics to evaluate the rigor of modeling solutions. Your expertise includes assessing the construction of passing networks (e.g., node/edge definition, weighting schemes for passes, handling of directionality and frequency), validation of structural indicators (centrality measures, clustering coefficients, dyadic/triadic motifs, team formation patterns), and alignment of network properties with game context. You will evaluate whether the solution appropriately explores multi-scale interactions (micro: pairwise passing frequency; meso: positional subgroups; macro: team-wide connectivity) and temporal dynamics (short-term: minute-to-minute network shifts; long-term: across games/season trends). Additionally, you will assess the rationality of performance indicators (e.g., diversity of pass types, coordination via network synchrony, contribution distribution entropy) by verifying their statistical or logical link to team success (e.g., correlation with goal creation, win probability). You will ensure conclusions about successful/unsuccessful strategies are grounded in data (e.g., comparing network structures of wins vs. losses, opponent-specific adaptations) and that the model for teamwork structurally and dynamically captures the coach’s requested features (configurational patterns, temporal flow)."
            },
            {
                "name": "Innovative Team Performance Modeler",
                "details": "As an Innovative Team Performance Modeler, you focus on identifying novel analytical approaches, integrative frameworks, and creative generalizations in the solution. Your expertise spans advanced network analytics (temporal network motifs, dynamic community detection), cross-disciplinary method integration (e.g., combining passing networks with event sequence analysis from fullevents.csv), and innovative performance metric design. You will evaluate whether the solution goes beyond standard network metrics (e.g., degree centrality) to propose unique indicators (e.g., 'adaptability score' as network resilience to opponent pressure, 'tempo' as passing rate variability, or 'flow' as uninterrupted subgraph persistence). You will assess if the model for teamwork dynamically links structural features (e.g., triadic passing motifs) to real-time game outcomes (e.g., shot opportunities) using innovative methods (e.g., temporal path analysis, machine learning to predict success from network snapshots). Additionally, you will judge the creativity of generalizing soccer-specific findings to broader team design (e.g., translating positional diversity in passing networks to role diversity in organizational teams) and whether the solution identifies gaps in current models (e.g., need for capturing communication quality or adaptive learning in teams) that advance the field of team effectiveness research."
            }
        ]
    },
    {
        "id": 29,
        "title": "A Wealth of Data",
        "background": "In the online marketplace it created, Amazon provides customers with an opportunity to rate and review purchases. Individual ratings - called “star ratings” – allow purchasers to express their level of satisfaction with a product using a scale of 1 (low rated, low satisfaction) to 5 (highly rated, high satisfaction). Additionally, customers can submit text-based messages – called “reviews” – that express further opinions and information about the product. Other customers can submit ratings on these reviews as being helpful or not – called a “helpfulness rating” – towards assisting their own product purchasing decision. Companies use these data to gain insights into the markets in which they participate, the timing of that participation, and the potential success of product design feature choices. \nSunshine Company is planning to introduce and sell three new products in the online marketplace: a microwave oven, a baby pacifier, and a hair dryer. They have hired your team as consultants to identify key patterns, relationships, measures, and parameters in past customersupplied ratings and reviews associated with other competing products to 1) inform their online sales strategy and 2) identify potentially important design features that would enhance product desirability. Sunshine Company has used data to inform sales strategies in the past, but they have not previously used this particular combination and type of data. Of particular interest to Sunshine Company are time-based patterns in these data, and whether they interact in ways that will help the company craft successful products. \nTo assist you, Sunshine’s data center has provided you with three data files for this project: hair_dryer.tsv, microwave.tsv, and pacifier.tsv. These data represent customer-supplied ratings and reviews for microwave ovens, baby pacifiers, and hair dryers sold in the Amazon marketplace over the time period(s) indicated in the data. A glossary of data label definitions is provided as well. THE DATA FILES PROVIDED CONTAIN THE ONLY DATA YOU SHOULD USE FOR THIS PROBLEM.",
        "problem_requirement": "Requirements \n1. Analyze the three product data sets provided to identify, describe, and support with mathematical evidence, meaningful quantitative and/or qualitative patterns, relationships, measures, and parameters within and between star ratings, reviews, and helpfulness ratings that will help Sunshine Company succeed in their three new online marketplace product offerings. \n2. Use your analysis to address the following specific questions and requests from the Sunshine Company Marketing Director: \n    a. Identify data measures based on ratings and reviews that are most informative for Sunshine Company to track, once their three products are placed on sale in the online marketplace. \n  b. Identify and discuss time-based measures and patterns within each data set that might suggest that a product’s reputation is increasing or decreasing in the online marketplace. \n    c. Determine combinations of text-based measure(s) and ratings-based measures that best indicate a potentially successful or failing product.\n   d. Do specific star ratings incite more reviews? For example, are customers more likely to write some type of review after seeing a series of low star ratings? \n  e. Are specific quality descriptors of text-based reviews such as ‘enthusiastic’, ‘disappointed’, and others, strongly associated with rating levels? \n3. Write a one- to two-page letter to the Marketing Director of Sunshine Company summarizing your team’s analysis and results. Include specific justification(s) for the result that your team most confidently recommends to the Marketing Director.",
        "dataset_path": [
            "hair_dryer.tsv",
            "microwave.tsv",
            "pacifier.tsv"
        ],
        "dataset_description": "Data Set Definitions: Each row represents data partitioned into the following columns.\\n● marketplace (string): 2 letter country code of the marketplace where the review was written.\\n● customer_id (string): Random identifier that can be used to aggregate reviews written by a single author.\\n● review_id (string): The unique ID of the review.\\n● product_id (string): The unique Product ID the review pertains to.\\n● product_parent (string): Random identifier that can be used to aggregate reviews for the same product.\\n● product_title (string): Title of the product.\\n● product_category (string): The major consumer category for the product.\\n● star_rating (int): The 1-5 star rating of the review.\\n● helpful_votes (int): Number of helpful votes.\\n● total_votes (int): Number of total votes the review received.\\n● vine (string): Customers are invited to become Amazon Vine Voices based on the trust that they have earned in the Amazon community for writing accurate and insightful reviews. Amazon provides Amazon Vine members with free copies of products that have been submitted to the program by vendors. Amazon doesn\"t influence the opinions of Amazon Vine members, nor do they modify or edit reviews.\\n● verified_purchase (string): A “Y” indicates Amazon verified that the person writing the review purchased the product at Amazon and didn\"t receive the product at a deep discount.\\n● review_headline (string): The title of the review.\\n● review_body (string): The review text.\\n● review_date (bigint): The date the review was written.",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 3,
        "source": "MCM2020c",
        "source_type": "MCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Quantitative & Qualitative Market Research Analyst",
                "details": "As a Quantitative & Qualitative Market Research Analyst, you specialize in integrating statistical rigor with qualitative insight to evaluate business data. Your expertise includes designing and assessing analyses that combine numerical metrics (e.g., star ratings, helpfulness votes) and text data (e.g., review content, descriptors), with a focus on validating mathematical evidence, logical reasoning, and alignment with business objectives. You will evaluate whether the solution thoroughly addresses all requirements: identifying meaningful patterns within/between star ratings, reviews, and helpfulness ratings; using appropriate statistical methods (e.g., regression, time-series analysis, hypothesis testing) to support claims; and ensuring conclusions about time-based trends, text-rating combinations, and review incitement are grounded in robust data interpretation. You will also check if the analysis logically connects findings to actionable recommendations for the Marketing Director (e.g., selecting 'most informative measures' or 'reputation indicators') with clear justification."
            },
            {
                "name": "Innovative Text Analytics & Strategic Pattern Recognition Specialist",
                "details": "As an Innovative Text Analytics & Strategic Pattern Recognition Specialist, you focus on evaluating the creativity and novelty of analytical approaches to uncover non-obvious insights from customer data. Your expertise spans advanced natural language processing (NLP), novel metric development, and time-series innovation, with a goal of identifying breakthrough connections between data types. You will assess whether the solution goes beyond standard analyses (e.g., basic sentiment analysis, simple time trends) by: 1) Using creative text-mining techniques (e.g., topic modeling with sentiment co-occurrence, identification of niche quality descriptors beyond 'enthusiastic'/'disappointed'); 2) Developing innovative combined metrics (e.g., temporal 'reputation velocity' indices, text-ratings interaction scores); or 3) Applying non-traditional time-series methods (e.g., change point detection for reputation shifts, seasonal decomposition of review patterns). You will determine if these innovations enhance strategic recommendations, providing Sunshine Company with unique competitive insights not achievable through conventional analysis."
            }
        ]
    },
    {
        "id": 30,
        "title": "The Opioid Crisis ",
        "background": "Background: The United States is experiencing a national crisis regarding the use of synthetic and nonsynthetic opioids, either for the treatment and management of pain (legal, prescription use) or for recreational purposes (illegal, non-prescription use). Federal organizations such as the Centers for Disease Control (CDC) are struggling to “save lives and prevent negative health effects of this epidemic, such as opioid use disorder, hepatitis, and HIV infections, and neonatal abstinence syndrome.”1 Simply enforcing existing laws is a complex challenge for the Federal Bureau of Investigation (FBI), and the U.S. Drug Enforcement Administration (DEA), among others.  \nThere are implications for important sectors of the U.S. economy as well. For example, if the opioid crisis spreads to all cross-sections of the U.S. population (including the college-educated and those with advanced degrees), businesses requiring precision labor skills, high technology component assembly, and sensitive trust or security relationships with clients and customers might have difficulty filling these positions. Further, if the percentage of people with opioid addiction increases within the elderly, health care costs and assisted living facility staffing will also be affected.  \nThe DEA/National Forensic Laboratory Information System (NFLIS), as part of the Drug Enforcement Administration's (DEA) Office of Diversion Control, publishes a data-heavy annual report addressing 'drug identification results and associated information from drug cases analyzed by federal, state, and local forensic laboratories.' The database within NFLIS includes data from crime laboratories that handle over 88% of the nation's estimated 1.2 million annual state and local drug cases. For this problem, we focus on the individual counties located in five (5) U.S. states: Ohio, Kentucky, West Virginia, Virginia, and Tennessee. In the U.S., a county is the next lower level of government below each state that has taxation authority. \nSupplied with this problem description are several data sets for your use. The first file (MCM_NFLIS_Data.xlsx) contains drug identification counts in years 2010-2017 for narcotic analgesics (synthetic opioids) and heroin in each of the counties from these five states as reported to the DEA by crime laboratories throughout each state. A drug identification occurs when evidence is submitted to crime laboratories by law enforcement agencies as part of a criminal investigation and the laboratory’s forensic scientists test the evidence. Typically, when law enforcement organizations submit these samples, they provide location data (county) with their incident reports. When evidence is submitted to a crime laboratory and this location data is not provided, the crime laboratory uses the location of the city/county/state investigating law enforcement organization that submitted the case. For the purposes of this problem, you may assume that the county location data are correct as provided. \nThe additional seven (7) files are zipped folders containing extracts from the U.S. Census Bureau that represent a common set of socio-economic factors collected for the counties of these five states during each of the years 2010-2016 (ACS_xx_5YR_DP02.zip). (Note: The same data were not available for 2017.)  \nA code sheet is present with each data set that defines each of the variables noted. While you may use other resources for research and background information, THE DATA SETS PROVIDED CONTAIN THE ONLY DATA YOU SHOULD USE FOR THIS PROBLEM.",
        "problem_requirement": "Problem: \nPart 1. Using the NFLIS data provided, build a mathematical model to describe the spread and characteristics of the reported synthetic opioid and heroin incidents (cases) in and between the five states and their counties over time. Using your model, identify any possible locations where specific opioid use might have started in each of the five states. \nIf the patterns and characteristics your team identified continue, are there any specific concerns the U.S. government should have? At what drug identification threshold levels do these occur? Where and when does your model predict they will occur in the future?  \nPart 2. Using the U.S. Census socio-economic data provided, address the following questions: \nThere are a good number of competing hypotheses that have been offered as explanations as to how opioid use got to its current level, who is using/abusing it, what contributes to the growth in opioid use and addiction, and why opioid use persists despite its known dangers. Is use or trends-in-use somehow associated with any of the U.S. Census socio-economic data provided? If so, modify your model from \nPart 1 to include any important factors from this data set. \nPart 3. Finally, using a combination of your Part 1 and Part 2 results, identify a possible strategy for countering the opioid crisis. Use your model(s) to test the effectiveness of this strategy; identifying any significant parameter bounds that success (or failure) is dependent upon. \nIn addition to your main report, include a 1-2 page memo to the Chief Administrator, DEA/NFLIS Database summarizing any significant insights or results you identified during this modeling effort.",
        "dataset_path": [
            "ACS_10_5YR_DP02.zip",
            "ACS_11_5YR_DP02.zip",
            "ACS_12_5YR_DP02.zip",
            "ACS_13_5YR_DP02.zip",
            "ACS_14_5YR_DP02.zip",
            "ACS_15_5YR_DP02.zip",
            "ACS_16_5YR_DP02.zip",
            "MCM_NFLIS_Data.xlsx"
        ],
        "dataset_description": "",
        "variable_description": [
            {}
        ],
        "addendum": "",
        "data_num": 8,
        "source": "MCM2019c",
        "source_type": "MCM",
        "role": [
            {
                "name": "statistical expert",
                "details":"As a statistical expert, you are proficient in probability theory, statistical inference, and data-driven modeling. You should focus on reviewing the statistical assumptions underlying the models, validating the appropriateness of the distributions used, and ensuring that estimation methods are unbiased and efficient. Pay attention to issues such as sample size adequacy, uncertainty quantification, and robustness of results under varying data conditions. Your expertise will ensure that the models are statistically valid and that the conclusions drawn are reliable and generalizable."
            },
            {
                "name": "data scientist",
                "details": "As a data scientist, you are proficient in data analysis, machine learning, and statistical methods. You should review the data-driven aspects of the modeling solutions, ensuring that the data is clean, relevant, and correctly interpreted. Focus on the use of predictive analytics to forecast wildfire occurrences and assess damage. Your role is to validate the data inputs and outputs, ensuring they are reliable and contribute to effective decision-making."
            },
            {
                "name": "Spatial-Temporal Epidemiological Model Analyst",
                "details": "As a Spatial-Temporal Epidemiological Model Analyst, you specialize in evaluating the rigor of models that track the spread of phenomena across geographic regions and time. Your expertise includes spatial statistics, temporal trend analysis, and the validation of origin-detection methods. You will assess how well solutions leverage NFLIS drug identification data to model opioid spread dynamics between counties and states (2010–2017), ensuring the reasoning for identifying 'start locations' is statistically robust (e.g., via cluster analysis, time-lagged correlation, or diffusion modeling). You will also evaluate the depth of analysis for threshold levels and future predictions, checking if they are grounded in validated model outputs (e.g., forecasting uncertainty bounds, sensitivity to data gaps) and if socioeconomic data integration (Part 2) logically enhances the model’s explanatory power (e.g., causal inference vs. correlation)."
            },
            {
                "name": "Innovative Public Health Data Integration Strategist",
                "details": "As an Innovative Public Health Data Integration Strategist, you focus on advancing modeling approaches by merging diverse datasets and designing creative, evidence-based interventions. Your expertise spans novel analytical methods (e.g., machine learning with spatial autocorrelation, Bayesian hierarchical modeling) and the translation of complex data into actionable strategies. You will evaluate the innovation in how solutions integrate U.S. Census socioeconomic data (2010–2016) with NFLIS drug data—assessing if they go beyond simple correlation to identify causal or predictive socioeconomic drivers (e.g., using LASSO regression, causal forests, or geographically weighted regression). Additionally, you will judge the originality of the counter-opioid strategy, ensuring it is not merely a standard enforcement/policy recommendation but is tailored to model-identified hotspots and socioeconomic vulnerabilities, with effectiveness tested via innovative parameter sensitivity analyses (e.g., agent-based simulations of intervention impact)."
            }
        ]
    }
]